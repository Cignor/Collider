================================================================================
MOOFY TIME-STRETCHING PATTERN GUIDE: Naive vs RubberBand Engine Selection
================================================================================

This document explains how to implement dual-engine time-stretching in audio
modules, allowing users to choose between:

1. RUBBERBAND: High-quality, phase-aware time-stretching using RubberBand library
   - Best for: Musical content, preserving transients, smooth pitch changes
   - Characteristics: Higher CPU usage, lower latency artifacts, phase-coherent
   
2. NAIVE: Simple linear interpolation-based time-stretching
   - Best for: Low CPU usage, real-time performance, simple use cases
   - Characteristics: Very low CPU, minimal latency, may introduce artifacts

================================================================================
ARCHITECTURE OVERVIEW
================================================================================

The pattern uses a unified TimePitchProcessor facade that internally manages
two separate engines:

  TimePitchProcessor
    ├─ RubberBandEngine (uses RubberBand library)
    └─ FifoEngine (naive linear interpolation)

Both engines share the same API:
  - setTimeStretchRatio(double ratio)
  - setPitchSemitones(double semis)
  - putInterleaved(input, frames)
  - receiveInterleaved(output, framesRequested)

The mode is switched via:
  - timePitch.setMode(TimePitchProcessor::Mode::RubberBand);
  - timePitch.setMode(TimePitchProcessor::Mode::Fifo);

================================================================================
IMPLEMENTATION STEPS
================================================================================

STEP 1: Add Engine Parameter to APVTS
  - Add AudioParameterChoice with values ["RubberBand", "Naive"]
  - Default typically 0 (RubberBand) or 1 (Naive) depending on preference
  - Example:
    parameters.push_back(std::make_unique<juce::AudioParameterChoice>(
        "engine", "Engine", juce::StringArray { "RubberBand", "Naive" }, 1));

STEP 2: Create/Update Time-Stretching Component
  - If wrapping AudioSource: Use TimeStretcherAudioSource pattern
  - If direct processing: Use TimePitchProcessor directly
  - Store TimePitchProcessor instance as member

STEP 3: Initialize in prepareToPlay()
  - Call timePitch.prepare(sampleRate, numChannels, blockSize)
  - Set initial mode based on parameter

STEP 4: Process Block - Branch by Engine
  Option A: Direct TimePitchProcessor (like TimePitchModuleProcessor)
    - Read engine parameter
    - Call timePitch.setMode() if changed
    - Use putInterleaved/receiveInterleaved pattern

  Option B: AudioSource Wrapper (like TimeStretcherAudioSource)
    - Add setMode() method to wrapper
    - Delegate mode setting to internal TimePitchProcessor
    - Update wrapper when engine parameter changes

STEP 5: Update UI
  - Add ComboBox/dropdown for engine selection
  - Optionally show RubberBand-specific options when RubberBand selected:
    - rbWindowShort (bool)
    - rbPhaseInd (bool)

STEP 6: Handle Mode Switching
  - Reset TimePitchProcessor when mode changes
  - Re-prime if needed (RubberBand needs initial frames)

================================================================================
KEY DIFFERENCES: RUBBERBAND vs NAIVE
================================================================================

RUBBERBAND (TimePitchProcessor::Mode::RubberBand):
  - Uses RubberBand::RubberBandStretcher library
  - Phase-aware processing
  - Better quality for musical content
  - Requires priming (initial frames) before output
  - Configurable options:
    * Window size (short/standard)
    * Phase independence
  - Higher CPU usage

NAIVE (TimePitchProcessor::Mode::Fifo):
  - Simple linear interpolation
  - Reads from FIFO buffer at variable rate
  - No priming needed
  - Lower CPU usage
  - May introduce artifacts on complex signals
  - Suitable for simple use cases

================================================================================
USAGE EXAMPLES
================================================================================

Example 1: SampleLoaderModuleProcessor Pattern
  - Uses SampleVoiceProcessor wrapper
  - SampleVoiceProcessor has Engine enum
  - Branches in renderBlock() based on engine
  - Naive path: Direct linear interpolation in renderBlock
  - RubberBand path: Uses TimePitchProcessor via interleaved buffers

Example 2: TimeStretcherAudioSource Pattern
  - Wraps PositionableAudioSource
  - Internal TimePitchProcessor handles both modes
  - Currently hardcoded to RubberBand (needs update to support selection)

Example 3: TimePitchModuleProcessor Pattern
  - Direct TimePitchProcessor usage
  - Engine selection via UI parameter
  - Mode switching handled in processBlock()

================================================================================
SECTION 6: VIDEO FILE LOADER MODULE (REQUIRES EXPERT REVIEW)
================================================================================

The VideoFileLoaderModule is experiencing buffer management and audio creaking
issues. This module:

- Uses FFmpegAudioReader to decode audio from video files
- Processes audio through time-stretching engines
- Has reported issues with:
  * Buffer underruns/overruns
  * Audio creaking/clicking artifacts
  * Synchronization problems

Files included for expert review:
  - VideoFileLoaderModule.h/cpp: Main module implementation
  - FFmpegAudioReader.h/cpp: FFmpeg audio decoding wrapper

================================================================================

================================================================================
FILE: juce\Source\audio\dsp\TimePitchProcessor.h
================================================================================


// Facade exposing a unified API with two independent engines
#pragma once
#include <juce_core/juce_core.h>
#include <cmath>
#include <rubberband/RubberBandStretcher.h>

class TimePitchProcessor
{
public:
    enum class Mode { RubberBand, Fifo };

    void setMode(Mode m) { mode = m; }

    void prepare (double sampleRate, int numChannels, int blockSize)
    {
        rb.prepare (sampleRate, numChannels, blockSize, optWindowShort, optPhaseIndependent);
        fifo.prepare (sampleRate, numChannels);
    }

    void reset()
    {
        rb.reset();
        fifo.reset();
    }

    void setTimeStretchRatio (double ratio)
    {
        rb.setTimeStretchRatio (ratio);
        fifo.setTimeStretchRatio (ratio);
    }

    void setPitchSemitones (double semis)
    {
        rb.setPitchSemitones (semis);
        fifo.setPitchSemitones (semis);
    }

    int putInterleaved (const float* inputLR, int frames)
    {
        return (mode == Mode::RubberBand) ? rb.putInterleaved (inputLR, frames)
                                          : fifo.putInterleaved (inputLR, frames);
    }

    int receiveInterleaved (float* outLR, int framesRequested)
    {
        return (mode == Mode::RubberBand) ? rb.receiveInterleaved (outLR, framesRequested)
                                          : fifo.receiveInterleaved (outLR, framesRequested);
    }

    int availableFrames() const
    {
        return (mode == Mode::RubberBand) ? rb.availableFrames() : fifo.availableFrames();
    }

    void setOptions (bool windowShort, bool phaseIndependent)
    {
        optWindowShort = windowShort; optPhaseIndependent = phaseIndependent;
    }

private:
    // RubberBand engine
    struct RubberBandEngine {
        void prepare (double sampleRate, int numChannels, int blockSize, bool windowShort, bool phaseInd)
        {
            sr = sampleRate; channels = juce::jmax (1, numChannels);
            using RB = RubberBand::RubberBandStretcher;
            int opts = RB::OptionProcessRealTime | RB::OptionPitchHighQuality | RB::OptionTransientsSmooth | RB::OptionPhaseIndependent |
                       (windowShort ? RB::OptionWindowShort : RB::OptionWindowStandard) |
                       (phaseInd ? RB::OptionPhaseIndependent : 0);
            stretcher = std::make_unique<RB>((size_t) sr, (size_t) channels, (RB::Options) opts);
            stretcher->setPitchScale(1.0); stretcher->setTimeRatio(1.0);
            if (blockSize > 0) stretcher->setMaxProcessSize ((size_t) blockSize);
            planarInput.setSize (channels, juce::jmax (1, blockSize));
            planarOutput.setSize (channels, juce::jmax (1, blockSize * 2));
        }
        void reset() { if (stretcher) stretcher->reset(); }
        void setTimeStretchRatio (double ratio) { if (stretcher) stretcher->setTimeRatio (juce::jlimit(0.25,4.0,ratio)); }
        void setPitchSemitones (double semis) { if (stretcher) stretcher->setPitchScale (std::pow(2.0, juce::jlimit(-24.0,24.0,semis)/12.0)); }
        int putInterleaved (const float* inputLR, int frames)
        {
            if (!stretcher || frames<=0 || channels<=0) return 0;
            if (planarInput.getNumSamples() < frames) planarInput.setSize (channels, frames, false, true, true);
            for (int ch = 0; ch < channels; ++ch)
            {
                float* dest = planarInput.getWritePointer (ch);
                for (int i = 0; i < frames; ++i) dest[i] = inputLR[i * channels + ch];
            }
            stretcher->process (planarInput.getArrayOfReadPointers(), (size_t) frames, false);
            return frames;
        }
        int receiveInterleaved (float* outLR, int framesRequested)
        {
            if (!stretcher || framesRequested<=0 || channels<=0) return 0;
            const size_t avail = stretcher->available();
            const int toGet = (int) juce::jmin<size_t>((size_t) framesRequested, avail);
            if (toGet<=0) return 0;
            if (planarOutput.getNumSamples() < toGet) planarOutput.setSize (channels, toGet, false, true, true);
            stretcher->retrieve (planarOutput.getArrayOfWritePointers(), (size_t) toGet);
            for (int ch = 0; ch < channels; ++ch)
            {
                const float* src = planarOutput.getReadPointer (ch);
                for (int i = 0; i < toGet; ++i) outLR[i * channels + ch] = src[i];
            }
            return toGet;
        }
        int availableFrames() const { return stretcher ? (int) stretcher->available() : 0; }

        double sr { 48000.0 }; int channels { 2 };
        std::unique_ptr<RubberBand::RubberBandStretcher> stretcher;
        juce::AudioBuffer<float> planarInput, planarOutput;
    } rb;

    // FIFO naive engine
    struct FifoEngine {
        void prepare (double sampleRate, int numChannels) { sr = sampleRate; channels = juce::jmax(1,numChannels); reset(); }
        void reset() { fifo.clearQuick(); readFramePos = 0.0; }
        void setTimeStretchRatio (double ratio) { timeRatio = (float) juce::jlimit(0.25,4.0,ratio); }
        void setPitchSemitones (double semis) { pitchSemi = (float) juce::jlimit(-24.0,24.0,semis); }
        int putInterleaved (const float* inputLR, int frames)
        {
            const int samples = frames * channels; const int start = fifo.size();
            fifo.resize (start + samples);
            std::memcpy (fifo.getRawDataPointer() + start, inputLR, (size_t) samples * sizeof (float));
            return frames;
        }
        int receiveInterleaved (float* outLR, int framesRequested)
        {
            if (channels<=0) channels=2;
            const double pitchFactor = std::pow (2.0, (double) pitchSemi / 12.0);
            const double stepFrames  = (1.0 / (double) juce::jmax (0.001f, timeRatio)) * pitchFactor;
            const int availableFrames = (int) (fifo.size() / channels);
            int framesWritten = 0; float* out = outLR;
            while (framesWritten < framesRequested)
            {
                const int baseIdx = (int) std::floor (readFramePos);
                if (baseIdx + 1 >= availableFrames) break;
                const double frac = readFramePos - (double) baseIdx;
                const int idx0 = baseIdx * channels; const int idx1 = (baseIdx + 1) * channels;
                for (int ch = 0; ch < channels; ++ch)
                { const float s0 = fifo[idx0 + ch]; const float s1 = fifo[idx1 + ch]; out[ch] = (float)((1.0 - frac)*s0 + frac*s1); }
                out += channels; ++framesWritten; readFramePos += stepFrames;
            }
            const int framesConsumed = (int) std::floor (readFramePos);
            if (framesConsumed > 0)
            { const int samplesConsumed = framesConsumed * channels; const int remainingSamples = (int) fifo.size() - samplesConsumed;
              if (remainingSamples > 0) std::memmove (fifo.getRawDataPointer(), fifo.getRawDataPointer() + samplesConsumed, (size_t) remainingSamples * sizeof (float));
              fifo.resize (remainingSamples); readFramePos -= (double) framesConsumed; }
            return framesWritten;
        }
        int availableFrames() const { return (int) (fifo.size() / juce::jmax(1,channels)); }

        double sr { 48000.0 }; int channels { 2 }; float timeRatio { 1.0f }; float pitchSemi { 0.0f };
        juce::Array<float> fifo; double readFramePos { 0.0 };
    } fifo;

    Mode mode { Mode::RubberBand };
    bool optWindowShort { true }; bool optPhaseIndependent { true };
};



================================================================================
FILE: juce\Source\audio\modules\SampleLoaderModuleProcessor.h
================================================================================


#pragma once

#include "../graph/ModularSynthProcessor.h"
#include "../assets/SampleBank.h"
#include "../voices/SampleVoiceProcessor.h"
#include "../dsp/TimePitchProcessor.h"
#include <juce_dsp/juce_dsp.h>
#include <juce_gui_basics/juce_gui_basics.h>

class SampleLoaderModuleProcessor : public ModuleProcessor
{
public:
    SampleLoaderModuleProcessor();
    ~SampleLoaderModuleProcessor() override = default;

    const juce::String getName() const override { return "sample_loader"; }

    // --- Audio Processing ---
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override {}
    void processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer& midiMessages) override;
    void reset() override;
    void getStateInformation (juce::MemoryBlock& destData) override;
    void setStateInformation (const void* data, int sizeInBytes) override;

    // --- Required by ModuleProcessor ---
    juce::AudioProcessorValueTreeState& getAPVTS() override { return apvts; }

    // --- Sample Loading ---
    void loadSample(const juce::File& file);
    void loadSample(const juce::String& filePath);
    void randomizeSample();
    juce::String getCurrentSampleName() const;
    bool hasSampleLoaded() const;

    // (Removed SoundTouch controls; using Rubber Band via TimePitchProcessor)
    
    // --- Debug and Monitoring ---
    void setDebugOutput(bool enabled);
    void logCurrentSettings() const;

    // --- Parameter Layout ---
    static juce::AudioProcessorValueTreeState::ParameterLayout createParameterLayout();

#if defined(PRESET_CREATOR_UI)
    void drawParametersInNode(float itemWidth, const std::function<bool(const juce::String& paramId)>& isParamModulated, const std::function<void()>& onModificationEnded) override;
    void drawIoPins(const NodePinHelpers& helpers) override;
#endif

    // Parameter bus contract implementation (must be available in Collider too)
    bool getParamRouting(const juce::String& paramId, int& outBusIndex, int& outChannelIndexInBus) const override;

    juce::String getAudioInputLabel(int channel) const override
    {
        switch (channel)
        {
            case 0: return "Pitch Mod";
            case 1: return "Speed Mod";
            case 2: return "Gate Mod";
            case 3: return "Trigger Mod";
            case 4: return "Range Start Mod";
            case 5: return "Range End Mod";
            case 6: return "Randomize Trig";
            default: return juce::String("In ") + juce::String(channel + 1);
        }
    }

    juce::String getAudioOutputLabel(int channel) const override
    {
        switch (channel)
        {
            case 0: return "Out L";
            case 1: return "Out R";
            default: return juce::String("Out ") + juce::String(channel + 1);
        }
    }

    std::vector<DynamicPinInfo> getDynamicOutputPins() const override
    {
        return {
            { "Out L", 0, PinDataType::Audio },
            { "Out R", 1, PinDataType::Audio }
        };
    }
    
    // CRITICAL: Accept multi-bus layout (like TTS Performer)
    bool isBusesLayoutSupported(const BusesLayout& layouts) const override
    {
        // Accept any layout as long as we have at least the minimum channels
        if (layouts.getMainInputChannelSet().isDisabled())
            return false;
        if (layouts.getMainOutputChannelSet().isDisabled())
            return false;
        return true;
    }
    
    // --- Spectrogram Access ---
    juce::Image getSpectrogramImage()
    {
        const juce::ScopedLock lock(imageLock);
        return spectrogramImage;
    }

private:
    // --- ADD THIS STATE VARIABLE ---
    std::atomic<bool> isPlaying { false };

    // --- APVTS ---
    juce::AudioProcessorValueTreeState apvts;
    std::atomic<float>* engineParam { nullptr }; // 0: rubberband, 1: naive
    
    // --- Sample Management ---
    std::shared_ptr<SampleBank::Sample> currentSample;
    std::unique_ptr<SampleVoiceProcessor> sampleProcessor;
    std::atomic<SampleVoiceProcessor*> newSampleProcessor { nullptr };
    juce::CriticalSection processorSwapLock;
    std::unique_ptr<SampleVoiceProcessor> processorToDelete;
    juce::String currentSampleName;
    juce::String currentSamplePath;
    
    // ADD THESE TWO LINES
    double sampleDurationSeconds = 0.0;
    int sampleSampleRate = 0;
    
    // Trigger edge detection for trigger_mod
    bool lastTriggerHigh { false };
    bool lastRandomizeTriggerHigh { false };
    
#if defined(PRESET_CREATOR_UI)
    // Keep a persistent chooser so async callback remains valid
    std::unique_ptr<juce::FileChooser> fileChooser;
#endif
    
    // Rubber Band is configured in TimePitchProcessor; keep no per-node ST params
    
    // --- Debug ---
    bool debugOutput { false };
    
    // --- Spectrogram Data ---
    juce::Image spectrogramImage;
    juce::CriticalSection imageLock;

    // --- Range Parameters ---
    std::atomic<float>* rangeStartParam { nullptr };
    std::atomic<float>* rangeEndParam { nullptr };
    std::atomic<float>* rangeStartModParam { nullptr };
    std::atomic<float>* rangeEndModParam { nullptr };
    double readPosition { 0.0 };
    
    // --- Parameter References ---
    // Parameters are accessed directly via apvts.getRawParameterValue()
    
    // --- Internal Methods ---
    void updateSoundTouchSettings();
    void createSampleProcessor();
    void generateSpectrogram();
};


================================================================================
FILE: juce\Source\audio\modules\SampleLoaderModuleProcessor.cpp
================================================================================


#include "SampleLoaderModuleProcessor.h"
#include <juce_audio_formats/juce_audio_formats.h>
#include <juce_gui_basics/juce_gui_basics.h>
#include "../../utils/RtLogger.h"

#if defined(PRESET_CREATOR_UI)
#include "../../preset_creator/ImGuiNodeEditorComponent.h"
#endif

SampleLoaderModuleProcessor::SampleLoaderModuleProcessor()
    : ModuleProcessor(BusesProperties()
        .withInput("Playback Mods", juce::AudioChannelSet::discreteChannels(2), true)  // Bus 0: Pitch, Speed (flat ch 0-1)
        .withInput("Control Mods", juce::AudioChannelSet::discreteChannels(2), true)   // Bus 1: Gate, Trigger (flat ch 2-3)
        .withInput("Range Mods", juce::AudioChannelSet::discreteChannels(2), true)     // Bus 2: Range Start, Range End (flat ch 4-5)
        .withInput("Randomize", juce::AudioChannelSet::discreteChannels(1), true)      // Bus 3: Randomize (flat ch 6)
        .withOutput("Audio Output", juce::AudioChannelSet::stereo(), true)),
      apvts(*this, nullptr, "SampleLoaderParameters", createParameterLayout())
{
    // Parameter references will be obtained when needed
    // Initialize output value tracking for cable inspector (stereo)
    lastOutputValues.clear();
    lastOutputValues.push_back(std::make_unique<std::atomic<float>>(0.0f));
    lastOutputValues.push_back(std::make_unique<std::atomic<float>>(0.0f));
    
    // Initialize parameter pointers
    rangeStartParam = apvts.getRawParameterValue("rangeStart");
    rangeEndParam = apvts.getRawParameterValue("rangeEnd");
    rangeStartModParam = apvts.getRawParameterValue("rangeStart_mod");
    rangeEndModParam = apvts.getRawParameterValue("rangeEnd_mod");
}



juce::AudioProcessorValueTreeState::ParameterLayout SampleLoaderModuleProcessor::createParameterLayout()
{
    std::vector<std::unique_ptr<juce::RangedAudioParameter>> parameters;
    
    // --- Basic Playback Parameters ---
    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "speed", "Speed", 0.25f, 4.0f, 1.0f));
    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "pitch", "Pitch (semitones)", -24.0f, 24.0f, 0.0f));
    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "gate", "Gate", 0.0f, 1.0f, 0.8f));
    parameters.push_back(std::make_unique<juce::AudioParameterChoice>(
        "engine", "Engine", juce::StringArray { "RubberBand", "Naive" }, 1));
    parameters.push_back(std::make_unique<juce::AudioParameterBool>(
        "rbWindowShort", "RB Window Short", true));
    parameters.push_back(std::make_unique<juce::AudioParameterBool>(
        "rbPhaseInd", "RB Phase Independent", true));
    parameters.push_back(std::make_unique<juce::AudioParameterBool>(
         "loop", "Loop", false));
    
    // (Removed legacy SoundTouch tuning parameters)

    // --- New Modulation Inputs (absolute control) ---
    // These live in APVTS and are fed by modulation cables; they override UI when connected.
    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "pitch_mod", "Pitch Mod", -24.0f, 24.0f, 0.0f));
    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "speed_mod", "Speed Mod", 0.25f, 4.0f, 1.0f));
    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "gate_mod", "Gate Mod", 0.0f, 1.0f, 1.0f));
    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "trigger_mod", "Trigger Mod", 0.0f, 1.0f, 0.0f));
    
    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "rangeStart_mod", "Range Start Mod", 0.0f, 1.0f, 0.0f));

    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "rangeEnd_mod", "Range End Mod", 0.0f, 1.0f, 1.0f));
    
    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "rangeStart", "Range Start", 
        juce::NormalisableRange<float>(0.0f, 1.0f), 0.0f));

    parameters.push_back(std::make_unique<juce::AudioParameterFloat>(
        "rangeEnd", "Range End", 
        juce::NormalisableRange<float>(0.0f, 1.0f), 1.0f));
    
    return { parameters.begin(), parameters.end() };
}

void SampleLoaderModuleProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    juce::ignoreUnused(sampleRate, samplesPerBlock);
    juce::Logger::writeToLog("[Sample Loader] prepareToPlay sr=" + juce::String(sampleRate) + ", block=" + juce::String(samplesPerBlock));
    
    // DEBUG: Check bus enablement status BEFORE forcing
    juce::String busStatusBefore = "[Sample Loader] Bus Status BEFORE: ";
    for (int i = 0; i < getBusCount(true); ++i)
    {
        auto* bus = getBus(true, i);
        if (bus)
            busStatusBefore += "In" + juce::String(i) + "=" + (bus->isEnabled() ? "ON" : "OFF") + "(" + juce::String(bus->getNumberOfChannels()) + "ch) ";
    }
    juce::Logger::writeToLog(busStatusBefore);
    
    // FORCE ENABLE ALL INPUT BUSES (AudioProcessorGraph might disable them)
    for (int i = 0; i < getBusCount(true); ++i)
    {
        if (auto* bus = getBus(true, i))
        {
            if (!bus->isEnabled())
            {
                enableAllBuses(); // Try to enable all
                juce::Logger::writeToLog("[Sample Loader] Forced all buses ON!");
                break;
            }
        }
    }
    
    // DEBUG: Check bus enablement status AFTER forcing
    juce::String busStatusAfter = "[Sample Loader] Bus Status AFTER: ";
    for (int i = 0; i < getBusCount(true); ++i)
    {
        auto* bus = getBus(true, i);
        if (bus)
            busStatusAfter += "In" + juce::String(i) + "=" + (bus->isEnabled() ? "ON" : "OFF") + "(" + juce::String(bus->getNumberOfChannels()) + "ch) ";
    }
    juce::Logger::writeToLog(busStatusAfter);
    
    // Auto-load sample from saved state if available
    if (currentSample == nullptr)
    {
        const auto savedPath = apvts.state.getProperty ("samplePath").toString();
        if (savedPath.isNotEmpty())
        {
            currentSamplePath = savedPath;
            loadSample (juce::File (currentSamplePath));
        }
    }
    // Create sample processor if we have a sample loaded
    if (currentSample != nullptr)
    {
        createSampleProcessor();
    }
}

void SampleLoaderModuleProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer& midiMessages)
{
    // Get OUTPUT bus, but do NOT clear here.
    // Clearing at the start can zero input buses when buffers are aliased in AudioProcessorGraph.
    auto outBus = getBusBuffer(buffer, false, 0);
    
    // --- Setup and Safety Checks ---
    if (auto* pending = newSampleProcessor.exchange(nullptr))
    {
        const juce::ScopedLock lock(processorSwapLock);
        processorToDelete = std::move(sampleProcessor);
        sampleProcessor.reset(pending);
    }
    SampleVoiceProcessor* currentProcessor = nullptr;
    {
        const juce::ScopedLock lock(processorSwapLock);
        currentProcessor = sampleProcessor.get();
    }
    if (currentProcessor == nullptr || currentSample == nullptr)
    {
        outBus.clear();
        return;
    }
    
    // DEBUG: Check MAIN buffer first (before splitting into buses)
    static int rawDebugCounter = 0;
    if (rawDebugCounter == 0 || rawDebugCounter % 240 == 0)
    {
        juce::String mainMsg = "[Sample MAIN Buffer #" + juce::String(rawDebugCounter) + "] ";
        mainMsg += "totalCh=" + juce::String(buffer.getNumChannels()) + " samples=" + juce::String(buffer.getNumSamples()) + " | ";
        
        // Check if buffer channels have valid pointers
        bool hasData = false;
        for (int ch = 0; ch < juce::jmin(7, buffer.getNumChannels()); ++ch)
        {
            float val = buffer.getSample(ch, 0);
            mainMsg += "ch" + juce::String(ch) + "=" + juce::String(val, 3) + " ";
            if (std::abs(val) > 0.001f) hasData = true;
        }
        
        mainMsg += "| hasData=" + juce::String(hasData ? "YES" : "NO");
        
        // Check channel pointer validity
        mainMsg += " | ptrs: ";
        for (int ch = 0; ch < juce::jmin(3, buffer.getNumChannels()); ++ch)
        {
            const float* ptr = buffer.getReadPointer(ch);
            mainMsg += "ch" + juce::String(ch) + "=" + (ptr ? "OK" : "NULL") + " ";
        }
        juce::Logger::writeToLog(mainMsg);
    }
    
    // Multi-bus input architecture (like TTS Performer)
    auto playbackBus = getBusBuffer(buffer, true, 0);  // Bus 0: Pitch, Speed (flat ch 0-1)
    auto controlBus = getBusBuffer(buffer, true, 1);   // Bus 1: Gate, Trigger (flat ch 2-3)
    auto rangeBus = getBusBuffer(buffer, true, 2);     // Bus 2: Range Start, Range End (flat ch 4-5)
    auto randomizeBus = getBusBuffer(buffer, true, 3); // Bus 3: Randomize (flat ch 6)
    
    // DEBUG: Check buses after splitting
    if (rawDebugCounter == 0 || rawDebugCounter % 240 == 0)
    {
        juce::String busMsg = "[Sample Buses #" + juce::String(rawDebugCounter) + "] ";
        busMsg += "playback=" + juce::String(playbackBus.getNumChannels()) + " ";
        busMsg += "control=" + juce::String(controlBus.getNumChannels()) + " ";
        busMsg += "range=" + juce::String(rangeBus.getNumChannels()) + " ";
        busMsg += "randomize=" + juce::String(randomizeBus.getNumChannels()) + " | ";
        if (playbackBus.getNumChannels() > 0) busMsg += "pitch=" + juce::String(playbackBus.getSample(0, 0), 3) + " ";
        if (playbackBus.getNumChannels() > 1) busMsg += "speed=" + juce::String(playbackBus.getSample(1, 0), 3) + " ";
        if (controlBus.getNumChannels() > 0) busMsg += "gate=" + juce::String(controlBus.getSample(0, 0), 3) + " ";
        juce::Logger::writeToLog(busMsg);
    }
    rawDebugCounter++;
    
    const int numSamples = buffer.getNumSamples();

    // --- Compute block-rate CV-mapped values for telemetry (even when not playing) ---
    const float baseSpeed = apvts.getRawParameterValue("speed")->load();
    float speedNow = baseSpeed;
    if (isParamInputConnected("speed_mod") && playbackBus.getNumChannels() > 1)
    {
        const float cv = juce::jlimit(0.0f, 1.0f, playbackBus.getReadPointer(1)[0]);
        const float octaveRange = 4.0f;
        const float octaveOffset = (cv - 0.5f) * octaveRange;
        speedNow = juce::jlimit(0.25f, 4.0f, baseSpeed * std::pow(2.0f, octaveOffset));
    }

    const float basePitch = apvts.getRawParameterValue("pitch")->load();
    float pitchNow = basePitch;
    if (isParamInputConnected("pitch_mod") && playbackBus.getNumChannels() > 0)
    {
        const float rawCV = playbackBus.getReadPointer(0)[0];
        const float bipolarCV = (rawCV >= 0.0f && rawCV <= 1.0f) ? (rawCV * 2.0f - 1.0f) : rawCV;
        const float pitchModulationRange = 24.0f; 
        pitchNow = juce::jlimit(-24.0f, 24.0f, basePitch + (bipolarCV * pitchModulationRange));
    }

    float startNorm = rangeStartParam->load();
    if (isParamInputConnected("rangeStart_mod") && rangeBus.getNumChannels() > 0)
        startNorm = juce::jlimit(0.0f, 1.0f, rangeBus.getReadPointer(0)[0]);

    float endNorm = rangeEndParam->load();
    if (isParamInputConnected("rangeEnd_mod") && rangeBus.getNumChannels() > 1)
        endNorm = juce::jlimit(0.0f, 1.0f, rangeBus.getReadPointer(1)[0]);

    // Ensure valid range window
    {
        const float minGap = 0.001f;
        if (startNorm >= endNorm)
        {
            const float midpoint = (startNorm + endNorm) * 0.5f;
            startNorm = juce::jlimit(0.0f, 1.0f - minGap, midpoint - minGap * 0.5f);
            endNorm   = juce::jlimit(minGap, 1.0f, startNorm + minGap);
        }
    }

    // Update live telemetry regardless of play state (matches TTS pattern)
    setLiveParamValue("speed_live", speedNow);
    setLiveParamValue("pitch_live", pitchNow);
    setLiveParamValue("rangeStart_live", startNorm);
    setLiveParamValue("rangeEnd_live", endNorm);
    // Gate live (use first sample if CV present, otherwise knob)
    if (isParamInputConnected("gate_mod") && controlBus.getNumChannels() > 0)
    {
        const float g = juce::jlimit(0.0f, 1.0f, controlBus.getReadPointer(0)[0]);
        setLiveParamValue("gate_live", g);
    }
    else
    {
        setLiveParamValue("gate_live", apvts.getRawParameterValue("gate")->load());
    }

    // --- 1. TRIGGER DETECTION ---
    const bool looping = apvts.getRawParameterValue("loop")->load() > 0.5f;
    
    // If loop is enabled and not playing, start playing
    if (looping && !currentProcessor->isPlaying)
    {
        currentProcessor->reset();
    }
    
    // Check for a rising edge on the trigger input to start playback.
    if (isParamInputConnected("trigger_mod") && controlBus.getNumChannels() > 1)
    {
        const float* trigSignal = controlBus.getReadPointer(1);  // Control Bus Channel 1 = Trigger
        for (int i = 0; i < numSamples; ++i)
        {
            const bool trigHigh = trigSignal[i] > 0.5f;
            if (trigHigh && !lastTriggerHigh)
            {
                reset(); // This now sets the internal voice's isPlaying to true
                break;
            }
            lastTriggerHigh = trigHigh;
        }
        if (numSamples > 0) lastTriggerHigh = (controlBus.getReadPointer(1)[numSamples - 1] > 0.5f);
    }

    // --- Randomize Trigger ---
    if (isParamInputConnected("randomize_mod") && randomizeBus.getNumChannels() > 0)
    {
        const float* randTrigSignal = randomizeBus.getReadPointer(0);  // Randomize Bus Channel 0
        for (int i = 0; i < numSamples; ++i)
        {
            const bool trigHigh = randTrigSignal[i] > 0.5f;
            if (trigHigh && !lastRandomizeTriggerHigh)
            {
                randomizeSample(); // Call the existing randomize function
                break; // Only randomize once per block
            }
            lastRandomizeTriggerHigh = trigHigh;
        }
        if (numSamples > 0) lastRandomizeTriggerHigh = (randomizeBus.getReadPointer(0)[numSamples - 1] > 0.5f);
    }

    // --- 2. CONDITIONAL AUDIO RENDERING ---
    // Only generate audio if the internal voice is in a playing state.
    if (currentProcessor->isPlaying)
    {
        // DEBUG: Log CV values from buses (like TTS)
        static int debugFrameCounter = 0;
        if (debugFrameCounter == 0 || debugFrameCounter % 240 == 0)
        {
            juce::String dbgMsg = "[Sample CV Debug #" + juce::String(debugFrameCounter) + "] ";
            if (playbackBus.getNumChannels() > 0) dbgMsg += "pitch_cv=" + juce::String(playbackBus.getReadPointer(0)[0], 3) + " ";
            if (playbackBus.getNumChannels() > 1) dbgMsg += "speed_cv=" + juce::String(playbackBus.getReadPointer(1)[0], 3) + " ";
            if (controlBus.getNumChannels() > 0) dbgMsg += "gate_cv=" + juce::String(controlBus.getReadPointer(0)[0], 3) + " ";
            juce::Logger::writeToLog(dbgMsg);
        }
        debugFrameCounter++;
        
        currentProcessor->setZoneTimeStretchRatio(speedNow);
        currentProcessor->setBasePitchSemitones(pitchNow);
        const int sourceLength = currentSample->stereo.getNumSamples();
        currentProcessor->setPlaybackRange(startNorm * sourceLength, endNorm * sourceLength);

        // Update APVTS parameters for UI feedback (especially spectrogram handles)
        *rangeStartParam = startNorm;
        apvts.getParameter("rangeStart")->sendValueChangedMessageToListeners(startNorm);
        *rangeEndParam = endNorm;
        apvts.getParameter("rangeEnd")->sendValueChangedMessageToListeners(endNorm);

        const int engineIdx = (int) apvts.getRawParameterValue("engine")->load();
        currentProcessor->setEngine(engineIdx == 0 ? SampleVoiceProcessor::Engine::RubberBand : SampleVoiceProcessor::Engine::Naive);
        currentProcessor->setRubberBandOptions(apvts.getRawParameterValue("rbWindowShort")->load() > 0.5f, apvts.getRawParameterValue("rbPhaseInd")->load() > 0.5f);
        currentProcessor->setLooping(apvts.getRawParameterValue("loop")->load() > 0.5f);

        // Generate the sample's audio into the OUTPUT buffer. This might set isPlaying to false if the sample ends.
        try {
            // Create a temporary buffer view for just the output bus
            juce::AudioBuffer<float> outputBuffer(outBus.getArrayOfWritePointers(), 
                                                   outBus.getNumChannels(), 
                                                   outBus.getNumSamples());
            currentProcessor->renderBlock(outputBuffer, midiMessages);
        } catch (...) {
            RtLogger::postf("[SampleLoader][FATAL] renderBlock exception");
            outBus.clear();
        }

        // --- 3. GATE (VCA) APPLICATION ---
        // If a gate is connected, use it to shape the volume of the audio we just generated.
        float lastGateValue = 1.0f;
        if (isParamInputConnected("gate_mod") && controlBus.getNumChannels() > 0)
        {
            const float* gateCV = controlBus.getReadPointer(0);  // Control Bus Channel 0 = Gate
            for (int ch = 0; ch < outBus.getNumChannels(); ++ch)
            {
                float* channelData = outBus.getWritePointer(ch);
                for (int i = 0; i < numSamples; ++i)
                {
                    const float gateValue = juce::jlimit(0.0f, 1.0f, gateCV[i]);
                    channelData[i] *= gateValue;
                    
                    // Update telemetry (throttled every 64 samples, only once per channel)
                    if (ch == 0 && (i & 0x3F) == 0)
                    {
                        setLiveParamValue("gate_live", gateValue);
                        lastGateValue = gateValue;
                    }
                }
            }
        }
        else
        {
            // No gate modulation - use static gate knob value
            lastGateValue = apvts.getRawParameterValue("gate")->load();
            setLiveParamValue("gate_live", lastGateValue);
        }
        
        // Apply main gate knob last
        outBus.applyGain(apvts.getRawParameterValue("gate")->load());
    }
    else
    {
        // Not playing: explicitly clear output now (safe after input analysis)
        outBus.clear();
    }
    
    // Update output values for cable inspector using block peak
    if (lastOutputValues.size() >= 2)
    {
        auto peakAbs = [&](int ch){ if (ch >= outBus.getNumChannels()) return 0.0f; const float* p = outBus.getReadPointer(ch); float m=0.0f; for (int i=0;i<outBus.getNumSamples();++i) m = juce::jmax(m, std::abs(p[i])); return m; };
        if (lastOutputValues[0]) lastOutputValues[0]->store(peakAbs(0));
        if (lastOutputValues[1]) lastOutputValues[1]->store(peakAbs(1));
    }
}

void SampleLoaderModuleProcessor::reset()
{
    if (sampleProcessor != nullptr)
    {
        sampleProcessor->reset();
    }
    
    if (currentSample != nullptr && rangeStartParam != nullptr)
    {
        readPosition = rangeStartParam->load() * currentSample->stereo.getNumSamples();
    }
    else
    {
        readPosition = 0.0;
    }
}

void SampleLoaderModuleProcessor::loadSample(const juce::File& file)
{
    if (!file.existsAsFile())
    {
        DBG("[Sample Loader] File does not exist: " + file.getFullPathName());
        return;
    }

    // 1) Load the original shared sample from the bank
    SampleBank sampleBank;
    std::shared_ptr<SampleBank::Sample> original;
    try {
        original = sampleBank.getOrLoad(file);
    } catch (...) {
        DBG("[Sample Loader][FATAL] Exception in SampleBank::getOrLoad");
        return;
    }
    if (original == nullptr || original->stereo.getNumSamples() <= 0)
    {
        DBG("[Sample Loader] Failed to load sample or empty: " + file.getFullPathName());
        return;
    }

    currentSampleName = file.getFileName();
    currentSamplePath = file.getFullPathName();
    apvts.state.setProperty ("samplePath", currentSamplePath, nullptr);

    // --- THIS IS THE FIX ---
    // Store the sample's metadata in our new member variables.
    sampleDurationSeconds = (double)original->stereo.getNumSamples() / original->sampleRate;
    sampleSampleRate = (int)original->sampleRate;
    // --- END OF FIX ---

    // 2) Create a private STEREO copy (preserve stereo or duplicate mono)
    auto privateCopy = std::make_shared<SampleBank::Sample>();
    privateCopy->sampleRate = original->sampleRate;
    const int numSamples = original->stereo.getNumSamples();
    privateCopy->stereo.setSize(2, numSamples); // Always stereo output

    if (original->stereo.getNumChannels() <= 1)
    {
        // Mono source: duplicate to both L and R channels
        privateCopy->stereo.copyFrom(0, 0, original->stereo, 0, 0, numSamples); // L = Mono
        privateCopy->stereo.copyFrom(1, 0, original->stereo, 0, 0, numSamples); // R = Mono
        DBG("[Sample Loader] Loaded mono sample and duplicated to stereo: " << file.getFileName());
    }
    else
    {
        // Stereo (or multi-channel) source: copy L and R channels
        privateCopy->stereo.copyFrom(0, 0, original->stereo, 0, 0, numSamples); // L channel
        privateCopy->stereo.copyFrom(1, 0, original->stereo, 1, 0, numSamples); // R channel
        DBG("[Sample Loader] Loaded stereo sample: " << file.getFileName());
    }

    // 3) Atomically assign our private copy for this module
    currentSample = privateCopy;
    generateSpectrogram();

    // 4) If the module is prepared, stage a new processor
    if (getSampleRate() > 0.0 && getBlockSize() > 0)
    {
        createSampleProcessor();
    }
    else
    {
        DBG("[Sample Loader][Defer] Module not prepared yet; will create processor in prepareToPlay");
    }
}
void SampleLoaderModuleProcessor::getStateInformation (juce::MemoryBlock& destData)
{
    juce::ValueTree vt ("SampleLoader");
    vt.setProperty ("samplePath", currentSamplePath, nullptr);
    vt.setProperty ("speed", apvts.getRawParameterValue("speed")->load(), nullptr);
    vt.setProperty ("pitch", apvts.getRawParameterValue("pitch")->load(), nullptr);
    vt.setProperty ("gate", apvts.getRawParameterValue("gate")->load(), nullptr);
    vt.setProperty ("engine", (int) apvts.getRawParameterValue("engine")->load(), nullptr);
    vt.setProperty ("rbWindowShort", apvts.getRawParameterValue("rbWindowShort")->load() > 0.5f, nullptr);
    vt.setProperty ("rbPhaseInd", apvts.getRawParameterValue("rbPhaseInd")->load() > 0.5f, nullptr);
    vt.setProperty ("loop", apvts.getRawParameterValue("loop")->load() > 0.5f, nullptr);
    if (auto xml = vt.createXml())
        copyXmlToBinary (*xml, destData);
}

void SampleLoaderModuleProcessor::setStateInformation (const void* data, int sizeInBytes)
{
    std::unique_ptr<juce::XmlElement> xml (getXmlFromBinary (data, sizeInBytes));
    if (! xml) return;
    juce::ValueTree vt = juce::ValueTree::fromXml (*xml);
    if (! vt.isValid()) return;
    currentSamplePath = vt.getProperty ("samplePath").toString();
    if (currentSamplePath.isNotEmpty())
        loadSample (juce::File (currentSamplePath));
    if (auto* p = apvts.getParameter ("speed"))
        p->setValueNotifyingHost (apvts.getParameterRange("speed").convertTo0to1 ((float) vt.getProperty ("speed", 1.0f)));
    if (auto* p = apvts.getParameter ("pitch"))
        p->setValueNotifyingHost (apvts.getParameterRange("pitch").convertTo0to1 ((float) vt.getProperty ("pitch", 0.0f)));
    if (auto* p = apvts.getParameter ("gate"))
        p->setValueNotifyingHost (apvts.getParameterRange("gate").convertTo0to1 ((float) vt.getProperty ("gate", 0.8f)));
    if (auto* p = apvts.getParameter ("engine"))
        p->setValueNotifyingHost ((float) (int) vt.getProperty ("engine", 0));
    if (auto* p = apvts.getParameter ("rbWindowShort"))
        p->setValueNotifyingHost ((bool) vt.getProperty ("rbWindowShort", true) ? 1.0f : 0.0f);
    if (auto* p = apvts.getParameter ("rbPhaseInd"))
        p->setValueNotifyingHost ((bool) vt.getProperty ("rbPhaseInd", true) ? 1.0f : 0.0f);
    if (auto* p = apvts.getParameter ("loop"))
        p->setValueNotifyingHost ((bool) vt.getProperty ("loop", false) ? 1.0f : 0.0f);
}

void SampleLoaderModuleProcessor::loadSample(const juce::String& filePath)
{
    loadSample(juce::File(filePath));
}

juce::String SampleLoaderModuleProcessor::getCurrentSampleName() const
{
    return currentSampleName;
}

bool SampleLoaderModuleProcessor::hasSampleLoaded() const
{
    return currentSample != nullptr;
}

// Legacy SoundTouch setters removed

void SampleLoaderModuleProcessor::setDebugOutput(bool enabled)
{
    debugOutput = enabled;
}

void SampleLoaderModuleProcessor::logCurrentSettings() const
{
    if (debugOutput)
    {
        DBG("[Sample Loader] Current Settings:");
        DBG("  Sample: " + currentSampleName);
        DBG("  Speed: " + juce::String(apvts.getRawParameterValue("speed")->load()));
        DBG("  Pitch: " + juce::String(apvts.getRawParameterValue("pitch")->load()));
    }
}

void SampleLoaderModuleProcessor::updateSoundTouchSettings() {}

void SampleLoaderModuleProcessor::randomizeSample()
{
    if (currentSamplePath.isEmpty())
        return;
        
    juce::File currentFile(currentSamplePath);
    juce::File parentDir = currentFile.getParentDirectory();
    
    if (!parentDir.exists() || !parentDir.isDirectory())
        return;
        
    // Get all audio files in the same directory
    juce::Array<juce::File> audioFiles;
    parentDir.findChildFiles(audioFiles, juce::File::findFiles, true, "*.wav;*.mp3;*.flac;*.aiff;*.ogg");
    
    if (audioFiles.size() <= 1)
        return;
        
    // Remove current file from the list
    for (int i = audioFiles.size() - 1; i >= 0; --i)
    {
        if (audioFiles[i].getFullPathName() == currentSamplePath)
        {
            audioFiles.remove(i);
            break;
        }
    }
    
    if (audioFiles.isEmpty())
        return;
        
    // Pick a random file
    juce::Random rng(juce::Time::getMillisecondCounterHiRes());
    juce::File randomFile = audioFiles[rng.nextInt(audioFiles.size())];
    
    DBG("[Sample Loader] Randomizing to: " + randomFile.getFullPathName());
    loadSample(randomFile);
}

void SampleLoaderModuleProcessor::createSampleProcessor()
{
    if (currentSample == nullptr)
    {
        return;
    }
    // Guard against double-creation and race with audio thread: build new then swap under lock
    auto newProcessor = std::make_unique<SampleVoiceProcessor>(currentSample);
    
    // Set up the sample processor
    const double sr = getSampleRate() > 0.0 ? getSampleRate() : 48000.0;
    const int bs = getBlockSize() > 0 ? getBlockSize() : 512;
    newProcessor->prepareToPlay(sr, bs);
    
    // --- Set initial playback range ---
    const float startNorm = rangeStartParam->load();
    const float endNorm = rangeEndParam->load();
    const double startSample = startNorm * currentSample->stereo.getNumSamples();
    const double endSample = endNorm * currentSample->stereo.getNumSamples();
    newProcessor->setPlaybackRange(startSample, endSample);
    newProcessor->resetPosition(); // Reset position without starting playback - wait for trigger
    
    // Set parameters from our APVTS
    newProcessor->setZoneTimeStretchRatio(apvts.getRawParameterValue("speed")->load());
    newProcessor->setBasePitchSemitones(apvts.getRawParameterValue("pitch")->load());
    newSampleProcessor.store(newProcessor.release());
    DBG("[Sample Loader] Staged new sample processor for: " << currentSampleName);
    
    DBG("[Sample Loader] Created sample processor for: " + currentSampleName);
}

void SampleLoaderModuleProcessor::generateSpectrogram()
{
    const juce::ScopedLock lock(imageLock);
    spectrogramImage = juce::Image(); // Clear previous image

    if (currentSample == nullptr || currentSample->stereo.getNumSamples() == 0)
        return;

    const int fftOrder = 10;
    const int fftSize = 1 << fftOrder;
    const int hopSize = fftSize / 4;
    const int numHops = (currentSample->stereo.getNumSamples() - fftSize) / hopSize;

    if (numHops <= 0) return;

    // Create a mono version for analysis if necessary
    juce::AudioBuffer<float> monoBuffer;
    if (currentSample->stereo.getNumChannels() > 1)
    {
        monoBuffer.setSize(1, currentSample->stereo.getNumSamples());
        monoBuffer.copyFrom(0, 0, currentSample->stereo, 0, 0, currentSample->stereo.getNumSamples());
        monoBuffer.addFrom(0, 0, currentSample->stereo, 1, 0, currentSample->stereo.getNumSamples(), 0.5f);
        monoBuffer.applyGain(0.5f);
    }
    const float* audioData = (currentSample->stereo.getNumChannels() > 1) ? monoBuffer.getReadPointer(0) : currentSample->stereo.getReadPointer(0);

    // Use RGB so JUCE's OpenGLTexture uploads with expected format
    spectrogramImage = juce::Image(juce::Image::RGB, numHops, fftSize / 2, true);
    juce::dsp::FFT fft(fftOrder);
    juce::dsp::WindowingFunction<float> window(fftSize, juce::dsp::WindowingFunction<float>::hann);
    std::vector<float> fftData(fftSize * 2);

    for (int i = 0; i < numHops; ++i)
    {
        std::fill(fftData.begin(), fftData.end(), 0.0f);
        memcpy(fftData.data(), audioData + (i * hopSize), fftSize * sizeof(float));

        window.multiplyWithWindowingTable(fftData.data(), fftSize);
        fft.performFrequencyOnlyForwardTransform(fftData.data());

        for (int j = 0; j < fftSize / 2; ++j)
        {
            const float db = juce::Decibels::gainToDecibels(juce::jmax(fftData[j], 1.0e-9f), -100.0f);
            float level = juce::jmap(db, -100.0f, 0.0f, 0.0f, 1.0f);
            level = juce::jlimit(0.0f, 1.0f, level);
            spectrogramImage.setPixelAt(i, (fftSize / 2) - 1 - j, juce::Colour::fromFloatRGBA(level, level, level, 1.0f));
        }
    }
}

#if defined(PRESET_CREATOR_UI)
void SampleLoaderModuleProcessor::drawParametersInNode(float itemWidth, const std::function<bool(const juce::String& paramId)>& isParamModulated, const std::function<void()>& onModificationEnded)
{
    // --- THIS IS THE DEFINITIVE FIX ---
    // 1. Draw all the parameter sliders and buttons FIRST.
    ImGui::PushItemWidth(itemWidth);

    if (ImGui::Button("Load Sample", ImVec2(itemWidth * 0.48f, 0)))
    {
        juce::File startDir;
        {
            auto appFile = juce::File::getSpecialLocation(juce::File::currentApplicationFile);
            auto dir = appFile.getParentDirectory();
            for (int i = 0; i < 8 && dir.exists(); ++i)
            {
                auto candidate = dir.getSiblingFile("audio").getChildFile("samples");
                if (candidate.exists() && candidate.isDirectory()) { startDir = candidate; break; }
                dir = dir.getParentDirectory();
            }
        }
        if (! startDir.exists()) startDir = juce::File();
        fileChooser = std::make_unique<juce::FileChooser>("Select Audio Sample", startDir, "*.wav;*.mp3;*.flac;*.aiff;*.ogg");
        auto chooserFlags = juce::FileBrowserComponent::openMode | juce::FileBrowserComponent::canSelectFiles;
        fileChooser->launchAsync(chooserFlags, [this](const juce::FileChooser& fc)
        {
            try {
                auto file = fc.getResult();
                if (file != juce::File{})
                {
                    juce::Logger::writeToLog("[Sample Loader] User selected file: " + file.getFullPathName());
                    loadSample(file);
                }
            } catch (...) {
                juce::Logger::writeToLog("[Sample Loader][FATAL] Exception during file chooser callback");
            }
        });
    }
    ImGui::SameLine();
    if (ImGui::Button("Random", ImVec2(itemWidth * 0.48f, 0))) { randomizeSample(); }

    // Range selection is now handled by the interactive spectrogram in the UI component

    ImGui::Spacing();
    // Main parameters in compact layout
    bool speedModulated = isParamModulated("speed_mod");
    if (speedModulated) { ImGui::BeginDisabled(); ImGui::PushStyleColor(ImGuiCol_FrameBg, ImVec4(1.0f, 1.0f, 0.0f, 0.3f)); }
    float speed = speedModulated ? getLiveParamValueFor("speed_mod", "speed_live", apvts.getRawParameterValue("speed")->load()) 
                                 : apvts.getRawParameterValue("speed")->load();
    if (ImGui::SliderFloat("Speed", &speed, 0.25f, 4.0f, "%.2fx"))
    {
        apvts.getParameter("speed")->setValueNotifyingHost(apvts.getParameterRange("speed").convertTo0to1(speed));
        onModificationEnded();
    }
    if (! speedModulated)
        ModuleProcessor::adjustParamOnWheel(apvts.getParameter("speed"), "speed", speed);
    if (speedModulated) { ImGui::PopStyleColor(); ImGui::EndDisabled(); }
    
    bool pitchModulated = isParamModulated("pitch_mod");
    if (pitchModulated) { ImGui::BeginDisabled(); ImGui::PushStyleColor(ImGuiCol_FrameBg, ImVec4(1.0f, 1.0f, 0.0f, 0.3f)); }
    float pitch = pitchModulated ? getLiveParamValueFor("pitch_mod", "pitch_live", apvts.getRawParameterValue("pitch")->load()) 
                                 : apvts.getRawParameterValue("pitch")->load();
    if (ImGui::SliderFloat("Pitch", &pitch, -24.0f, 24.0f, "%.1f st"))
    {
        apvts.getParameter("pitch")->setValueNotifyingHost(apvts.getParameterRange("pitch").convertTo0to1(pitch));
        onModificationEnded();
    }
    if (! pitchModulated)
        ModuleProcessor::adjustParamOnWheel(apvts.getParameter("pitch"), "pitch", pitch);
    if (pitchModulated) { ImGui::PopStyleColor(); ImGui::EndDisabled(); }
    
    // --- Gate slider (formerly volume) ---
    bool gateModulated = isParamModulated("gate_mod"); 
    if (gateModulated) { ImGui::BeginDisabled(); ImGui::PushStyleColor(ImGuiCol_FrameBg, ImVec4(1.0f, 1.0f, 0.0f, 0.3f)); }
    float gate = gateModulated ? getLiveParamValueFor("gate_mod", "gate_live", apvts.getRawParameterValue("gate")->load())
                               : apvts.getRawParameterValue("gate")->load();
    if (ImGui::SliderFloat("Gate", &gate, 0.0f, 1.0f, "%.2f"))
    {
        if (!gateModulated) {
            apvts.getParameter("gate")->setValueNotifyingHost(apvts.getParameterRange("gate").convertTo0to1(gate));
            onModificationEnded();
        }
    }
    if (!gateModulated)
        ModuleProcessor::adjustParamOnWheel(apvts.getParameter("gate"), "gate", gate);
    if (gateModulated) { ImGui::PopStyleColor(); ImGui::EndDisabled(); ImGui::SameLine(); ImGui::TextUnformatted("(mod)"); }
    
    // Range parameters with live modulation feedback
    bool rangeStartModulated = isParamModulated("rangeStart_mod");
    if (rangeStartModulated) { ImGui::BeginDisabled(); ImGui::PushStyleColor(ImGuiCol_FrameBg, ImVec4(1.0f, 1.0f, 0.0f, 0.3f)); }
    float rangeStart = rangeStartModulated ? getLiveParamValueFor("rangeStart_mod", "rangeStart_live", rangeStartParam->load()) 
                                          : rangeStartParam->load();
    float rangeEnd = rangeEndParam->load();
    if (ImGui::SliderFloat("Range Start", &rangeStart, 0.0f, 1.0f, "%.3f"))
    {
        // Ensure start doesn't exceed end (leave at least 0.001 gap)
        rangeStart = juce::jmin(rangeStart, rangeEnd - 0.001f);
        apvts.getParameter("rangeStart")->setValueNotifyingHost(apvts.getParameterRange("rangeStart").convertTo0to1(rangeStart));
        onModificationEnded();
    }
    if (! rangeStartModulated)
        ModuleProcessor::adjustParamOnWheel(apvts.getParameter("rangeStart"), "rangeStart", rangeStart);
    if (rangeStartModulated) { ImGui::PopStyleColor(); ImGui::EndDisabled(); }
    
    bool rangeEndModulated = isParamModulated("rangeEnd_mod");
    if (rangeEndModulated) { ImGui::BeginDisabled(); ImGui::PushStyleColor(ImGuiCol_FrameBg, ImVec4(1.0f, 1.0f, 0.0f, 0.3f)); }
    rangeEnd = rangeEndModulated ? getLiveParamValueFor("rangeEnd_mod", "rangeEnd_live", rangeEndParam->load()) 
                                 : rangeEndParam->load();
    rangeStart = rangeStartParam->load(); // Refresh rangeStart for validation
    if (ImGui::SliderFloat("Range End", &rangeEnd, 0.0f, 1.0f, "%.3f"))
    {
        // Ensure end doesn't go below start (leave at least 0.001 gap)
        rangeEnd = juce::jmax(rangeEnd, rangeStart + 0.001f);
        apvts.getParameter("rangeEnd")->setValueNotifyingHost(apvts.getParameterRange("rangeEnd").convertTo0to1(rangeEnd));
        onModificationEnded();
    }
    if (! rangeEndModulated)
        ModuleProcessor::adjustParamOnWheel(apvts.getParameter("rangeEnd"), "rangeEnd", rangeEnd);
    if (rangeEndModulated) { ImGui::PopStyleColor(); ImGui::EndDisabled(); }
    
    bool loop = apvts.getRawParameterValue("loop")->load() > 0.5f;
    if (ImGui::Checkbox("Loop", &loop))
    {
        apvts.getParameter("loop")->setValueNotifyingHost(loop ? 1.0f : 0.0f);
        onModificationEnded();
    }
    
    int engineIdx = (int) apvts.getRawParameterValue("engine")->load();
    const char* items[] = { "RubberBand", "Naive" };
    if (ImGui::Combo("Engine", &engineIdx, items, 2))
    {
        apvts.getParameter("engine")->setValueNotifyingHost((float) engineIdx);
        if (sampleProcessor)
            sampleProcessor->setEngine(engineIdx == 0 ? SampleVoiceProcessor::Engine::RubberBand
                                                      : SampleVoiceProcessor::Engine::Naive);
        onModificationEnded();
    }
    
    if (engineIdx == 0)
    {
        bool winShort = apvts.getRawParameterValue("rbWindowShort")->load() > 0.5f;
        if (ImGui::Checkbox("RB Window Short", &winShort))
        {
            apvts.getParameter("rbWindowShort")->setValueNotifyingHost(winShort ? 1.0f : 0.0f);
            if (sampleProcessor) sampleProcessor->setRubberBandOptions(winShort, apvts.getRawParameterValue("rbPhaseInd")->load() > 0.5f);
            onModificationEnded();
        }
        bool phaseInd = apvts.getRawParameterValue("rbPhaseInd")->load() > 0.5f;
        if (ImGui::Checkbox("RB Phase Independent", &phaseInd))
        {
            apvts.getParameter("rbPhaseInd")->setValueNotifyingHost(phaseInd ? 1.0f : 0.0f);
            if (sampleProcessor) sampleProcessor->setRubberBandOptions(apvts.getRawParameterValue("rbWindowShort")->load() > 0.5f, phaseInd);
            onModificationEnded();
        }
    }
    
    ImGui::PopItemWidth();
    
    // 2. Now, draw the sample information and visual display AT THE END.
    if (hasSampleLoaded())
    {
        ImGui::Text("Sample: %s", currentSampleName.toRawUTF8());
        ImGui::Text("Duration: %.2f s", sampleDurationSeconds);
        ImGui::Text("Rate: %d Hz", sampleSampleRate);

        // Draw a drop zone for hot-swapping with visual feedback
        ImVec2 swapZoneSize = ImVec2(itemWidth, 100.0f);
        
        // Check if a drag-drop operation is in progress
        bool isDragging = ImGui::GetDragDropPayload() != nullptr;
        
        if (isDragging)
        {
            // Beautiful blinking animation during drag-drop
            float time = (float)ImGui::GetTime();
            float pulse = (std::sin(time * 8.0f) * 0.5f + 0.5f); // Fast blink
            float glow = (std::sin(time * 3.0f) * 0.3f + 0.7f);  // Slower glow
            
            // Vibrant cyan with pulsing alpha
            ImU32 fillColor = IM_COL32(0, (int)(180 * glow), (int)(220 * glow), (int)(100 + pulse * 155));
            ImU32 borderColor = IM_COL32((int)(100 * glow), (int)(255 * pulse), (int)(255 * pulse), 255);
            
            ImGui::PushStyleColor(ImGuiCol_Button, fillColor);
            ImGui::PushStyleColor(ImGuiCol_Border, borderColor);
            ImGui::PushStyleVar(ImGuiStyleVar_FrameBorderSize, 3.0f);
            ImGui::Button("##dropzone_sample_swap", swapZoneSize);
            ImGui::PopStyleVar();
            ImGui::PopStyleColor(2);
        }
        else
        {
            // Discrete outline only when idle
            ImGui::PushStyleColor(ImGuiCol_Button, IM_COL32(0, 0, 0, 0)); // Transparent fill
            ImGui::PushStyleColor(ImGuiCol_Border, IM_COL32(100, 100, 100, 120)); // Gray outline
            ImGui::PushStyleVar(ImGuiStyleVar_FrameBorderSize, 1.0f);
            ImGui::Button("##dropzone_sample_swap", swapZoneSize);
            ImGui::PopStyleVar();
            ImGui::PopStyleColor(2);
        }
        
        // Draw text centered on the button
        const char* text = isDragging ? "Drop to Swap!" : "Drop to Swap Sample";
        ImVec2 textSize = ImGui::CalcTextSize(text);
        ImVec2 textPos = ImGui::GetItemRectMin();
        textPos.x += (swapZoneSize.x - textSize.x) * 0.5f;
        textPos.y += (swapZoneSize.y - textSize.y) * 0.5f;
        ImU32 textColor = isDragging ? IM_COL32(100, 255, 255, 255) : IM_COL32(150, 150, 150, 200);
        ImGui::GetWindowDrawList()->AddText(textPos, textColor, text);

        // 3. Make this button the drop target for hot-swapping.
        if (ImGui::BeginDragDropTarget())
        {
            if (const ImGuiPayload* payload = ImGui::AcceptDragDropPayload("DND_SAMPLE_PATH"))
            {
                const char* path = (const char*)payload->Data;
                loadSample(juce::File(path));
                onModificationEnded();
            }
            ImGui::EndDragDropTarget();
        }
    }
    else
    {
        // If NO sample is loaded, draw a dedicated dropzone with visual feedback
        ImVec2 dropZoneSize = ImVec2(itemWidth, 60.0f);
        
        // Check if a drag-drop operation is in progress
        bool isDragging = ImGui::GetDragDropPayload() != nullptr;
        
        if (isDragging)
        {
            // Beautiful blinking animation during drag-drop
            float time = (float)ImGui::GetTime();
            float pulse = (std::sin(time * 8.0f) * 0.5f + 0.5f); // Fast blink
            float glow = (std::sin(time * 3.0f) * 0.3f + 0.7f);  // Slower glow
            
            // Vibrant cyan with pulsing alpha
            ImU32 fillColor = IM_COL32(0, (int)(180 * glow), (int)(220 * glow), (int)(100 + pulse * 155));
            ImU32 borderColor = IM_COL32((int)(100 * glow), (int)(255 * pulse), (int)(255 * pulse), 255);
            
            ImGui::PushStyleColor(ImGuiCol_Button, fillColor);
            ImGui::PushStyleColor(ImGuiCol_Border, borderColor);
            ImGui::PushStyleVar(ImGuiStyleVar_FrameBorderSize, 3.0f);
            ImGui::Button("##dropzone_sample", dropZoneSize);
            ImGui::PopStyleVar();
            ImGui::PopStyleColor(2);
        }
        else
        {
            // Discrete outline only when idle
            ImGui::PushStyleColor(ImGuiCol_Button, IM_COL32(0, 0, 0, 0)); // Transparent fill
            ImGui::PushStyleColor(ImGuiCol_Border, IM_COL32(100, 100, 100, 120)); // Gray outline
            ImGui::PushStyleVar(ImGuiStyleVar_FrameBorderSize, 1.0f);
            ImGui::Button("##dropzone_sample", dropZoneSize);
            ImGui::PopStyleVar();
            ImGui::PopStyleColor(2);
        }
        
        // Draw text centered on top of the button
        const char* text = isDragging ? "Drop Here!" : "Drop Sample Here";
        ImVec2 textSize = ImGui::CalcTextSize(text);
        ImVec2 textPos = ImGui::GetItemRectMin();
        textPos.x += (dropZoneSize.x - textSize.x) * 0.5f;
        textPos.y += (dropZoneSize.y - textSize.y) * 0.5f;
        ImU32 textColor = isDragging ? IM_COL32(100, 255, 255, 255) : IM_COL32(150, 150, 150, 200);
        ImGui::GetWindowDrawList()->AddText(textPos, textColor, text);

        // Make THIS BUTTON the drop target.
        if (ImGui::BeginDragDropTarget())
        {
            if (const ImGuiPayload* payload = ImGui::AcceptDragDropPayload("DND_SAMPLE_PATH"))
            {
                const char* path = (const char*)payload->Data;
                loadSample(juce::File(path));
                onModificationEnded();
            }
            ImGui::EndDragDropTarget();
        }
    }
    // --- END OF FIX ---
}

void SampleLoaderModuleProcessor::drawIoPins(const NodePinHelpers& helpers)
{
    // Modulation inputs
    helpers.drawAudioInputPin("Pitch Mod", 0);
    helpers.drawAudioInputPin("Speed Mod", 1);
    helpers.drawAudioInputPin("Gate Mod", 2);
    helpers.drawAudioInputPin("Trigger Mod", 3);
    helpers.drawAudioInputPin("Range Start Mod", 4);
    helpers.drawAudioInputPin("Range End Mod", 5);
    helpers.drawAudioInputPin("Randomize Trig", 6);
    // Audio outputs (stereo)
    helpers.drawAudioOutputPin("Out L", 0);
    helpers.drawAudioOutputPin("Out R", 1);
}
#endif

// Parameter bus contract implementation (multi-bus architecture like TTS Performer)
bool SampleLoaderModuleProcessor::getParamRouting(const juce::String& paramId, int& outBusIndex, int& outChannelIndexInBus) const
{
    // Bus 0: Playback Mods (Pitch, Speed) - flat channels 0-1
    if (paramId == "pitch_mod") { outBusIndex = 0; outChannelIndexInBus = 0; return true; }
    if (paramId == "speed_mod") { outBusIndex = 0; outChannelIndexInBus = 1; return true; }
    
    // Bus 1: Control Mods (Gate, Trigger) - flat channels 2-3
    if (paramId == "gate_mod") { outBusIndex = 1; outChannelIndexInBus = 0; return true; }
    if (paramId == "trigger_mod") { outBusIndex = 1; outChannelIndexInBus = 1; return true; }
    
    // Bus 2: Range Mods (Range Start, Range End) - flat channels 4-5
    if (paramId == "rangeStart_mod") { outBusIndex = 2; outChannelIndexInBus = 0; return true; }
    if (paramId == "rangeEnd_mod") { outBusIndex = 2; outChannelIndexInBus = 1; return true; }
    
    // Bus 3: Randomize - flat channel 6
    if (paramId == "randomize_mod") { outBusIndex = 3; outChannelIndexInBus = 0; return true; }
    
    return false;
}

================================================================================
FILE: juce\Source\audio\voices\SampleVoiceProcessor.h
================================================================================


#pragma once
#include "../graph/VoiceProcessor.h"
#include <atomic>
#include "../assets/SampleBank.h"
#include "../dsp/TimePitchProcessor.h"

class SampleVoiceProcessor : public VoiceProcessor
{
public:
    SampleVoiceProcessor(std::shared_ptr<SampleBank::Sample> sampleToPlay);
    void prepareToPlay (double sampleRate, int samplesPerBlock) override;
    void renderBlock (juce::AudioBuffer<float>&, juce::MidiBuffer&) override;
    void reset() override { readPosition = startSamplePos; timePitch.reset(); isPlaying = true; }
    void resetPosition() { readPosition = startSamplePos; timePitch.reset(); } // Reset position without starting playback
    void setLooping (bool shouldLoop) { isLooping = shouldLoop; }
    void setBasePitchSemitones (float semitones) { basePitchSemitones = semitones; }
    void setZoneTimeStretchRatio (float ratio) { zoneTimeStretchRatio = juce::jlimit (0.25f, 4.0f, ratio); }
    void setSourceName (const juce::String& name) { sourceName = name; }
    juce::String getSourceName () const { return sourceName; }

    // Smoothing controls
    void setSmoothingEnabled (bool enabled) { requestedSmoothingEnabled.store(enabled, std::memory_order_relaxed); }
    void setSmoothingTimeMs (float timeMs, float pitchMs) { smoothingTimeMsTime = timeMs; smoothingTimeMsPitch = pitchMs; }
    void setSmoothingAlpha (float alphaTime, float alphaPitch) { smoothingAlphaTime = alphaTime; smoothingAlphaPitch = alphaPitch; }
    void setSmoothingMaxBlocks (int maxBlocksTime, int maxBlocksPitch) { smoothingMaxBlocksTime = juce::jmax(1, maxBlocksTime); smoothingMaxBlocksPitch = juce::jmax(1, maxBlocksPitch); }
    void setSmoothingSnapThresholds (float timeRatioDelta, float pitchSemisDelta) { smoothingSnapThresholdTime = timeRatioDelta; smoothingSnapThresholdPitch = pitchSemisDelta; }
    void setSmoothingResetPolicy (bool resetOnLargeChange, bool resetWhenNoSmoothing) { resetOnSnap = resetOnLargeChange; resetOnChangeWhenNoSmoothing = resetWhenNoSmoothing; }

    // Engine selection
    enum class Engine { RubberBand = 0, Naive = 1 };
    void setEngine (Engine e)
    {
        Engine current = engine.load(std::memory_order_relaxed);
        if (current == e) return; // avoid resetting every block
        engine.store(e, std::memory_order_relaxed);
        timePitch.setMode(e==Engine::RubberBand? TimePitchProcessor::Mode::RubberBand : TimePitchProcessor::Mode::Fifo);
        timePitch.reset();
    }
    void setRubberBandOptions (bool windowShort, bool phaseIndependent) { timePitch.setOptions(windowShort, phaseIndependent); }
    
    void setPlaybackRange(double startSample, double endSample)
    {
        startSamplePos = startSample;
        endSamplePos = endSample;
    }

public:
    bool isLooping { true };
    bool isPlaying { false }; // MOVED TO PUBLIC

private:
    std::shared_ptr<SampleBank::Sample> sourceSample;
    juce::String sourceName;
    double readPosition { 0.0 };
    double outputSampleRate { 48000.0 };
    float basePitchSemitones { 0.0f }; // grid-based pitch at spawn
    float zoneTimeStretchRatio { 1.0f }; // per-voice, dynamic (zones)
    float independentPitchRatio { 1.0f }; // pitch factor that does not affect tempo
    TimePitchProcessor timePitch; // placeholder; swap to SoundTouch impl later
    juce::HeapBlock<float> interleavedInput;
    double startSamplePos { 0.0 };
    double endSamplePos { -1.0 }; // -1 indicates playback to the end of the sample
    juce::HeapBlock<float> interleavedOutput;
    int interleavedCapacityFrames { 0 };
    float lastEffectiveTime { 1.0f };
    float lastEffectivePitchSemis { 0.0f };
    // Smooth parameter transitions (independent for time & pitch)
    float smoothedTimeRatio { 1.0f };
    float smoothedPitchSemis { 0.0f };
    int   smoothingBlocksRemainingTime { 0 };
    int   smoothingBlocksRemainingPitch { 0 };
    float timeStepPerBlock { 0.0f };
    float pitchStepPerBlock { 0.0f };
    float smoothingTimeMsTime { 100.0f };  // defaults per user preference
    float smoothingTimeMsPitch { 100.0f };
    bool  smoothingEnabled { true };
    float smoothingAlphaTime { 0.4f };
    float smoothingAlphaPitch { 0.4f };
    int   smoothingMaxBlocksTime { 1 };
    int   smoothingMaxBlocksPitch { 1 };
    float smoothingSnapThresholdTime { 0.5f };    // time ratio delta to snap
    float smoothingSnapThresholdPitch { 3.0f };   // semitone delta to snap
    bool  resetOnSnap { true };
    bool  resetOnChangeWhenNoSmoothing { true };
    std::atomic<bool> requestedSmoothingEnabled { true };

    std::atomic<Engine> engine { Engine::RubberBand };
    // Fast handover state
    bool  inBypassHandover { false };
    int   handoverFramesRemaining { 0 };
    int   handoverCrossfadeFrames { 240 };
};

================================================================================
FILE: juce\Source\audio\voices\SampleVoiceProcessor.cpp
================================================================================


#include "SampleVoiceProcessor.h"
#include "../../utils/RtLogger.h"

SampleVoiceProcessor::SampleVoiceProcessor(std::shared_ptr<SampleBank::Sample> sampleToPlay)
    : sourceSample(std::move(sampleToPlay))
{
}

void SampleVoiceProcessor::prepareToPlay(double rate, int samplesPerBlock)
{
    // Prepare base FX chain, then set sample-rate specific state
    VoiceProcessor::prepareToPlay (rate, samplesPerBlock);
    juce::Logger::writeToLog("[SampleVoice] prepareToPlay sr=" + juce::String(rate) + ", block=" + juce::String(samplesPerBlock));
    outputSampleRate = rate;
    // readPosition is set by reset() or setPlaybackRange() + reset(), not here

    // Always run stretcher in stereo; duplicate mono content upstream
    timePitch.prepare (rate, 2, samplesPerBlock);
    interleavedCapacityFrames = samplesPerBlock;
    interleavedInput.allocate ((size_t) (interleavedCapacityFrames * 2), true);
    interleavedOutput.allocate ((size_t) (interleavedCapacityFrames * 2), true);
    // Reset smoothing state
    smoothedTimeRatio = 1.0f;
    smoothedPitchSemis = 0.0f;
    smoothingBlocksRemainingTime = 0;
    smoothingBlocksRemainingPitch = 0;

    // Defaults per current preferred settings
    setSmoothingEnabled (true);
    setSmoothingTimeMs (100.0f, 100.0f);
    setSmoothingAlpha (0.4f, 0.4f);
    setSmoothingMaxBlocks (1, 1);
    setSmoothingSnapThresholds (0.5f, 3.0f);
    setSmoothingResetPolicy (true, true);
}

void SampleVoiceProcessor::renderBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer& midi)
{
    juce::ignoreUnused(midi);
    // buffer.clear(); // REMOVED: This was wiping out the audio before processing

    if (sourceSample == nullptr || sourceSample->stereo.getNumSamples() < 2 || outputSampleRate <= 0.0)
        return;

    const int numDestSamples = buffer.getNumSamples();
    auto& sourceBuffer = sourceSample->stereo;
    const int sourceLength = sourceBuffer.getNumSamples();

    // Apply UI smoothing toggle atomically at audio rate
    smoothingEnabled = requestedSmoothingEnabled.load(std::memory_order_relaxed);

    // Parameters to stretcher
    const float apTime = apvts.getRawParameterValue("timeStretchRatio") ? apvts.getRawParameterValue("timeStretchRatio")->load() : 1.0f;
    const float apPitch = apvts.getRawParameterValue("pitchSemitones") ? apvts.getRawParameterValue("pitchSemitones")->load() : 0.0f;
    const float effectiveTime = juce::jlimit(0.25f, 4.0f, apTime * zoneTimeStretchRatio);
    const float effectivePitchSemis = basePitchSemitones + apPitch;

    // If smoothing is disabled, apply parameters immediately
    if (!smoothingEnabled)
    { smoothedTimeRatio = effectiveTime; smoothedPitchSemis = effectivePitchSemis; if (resetOnChangeWhenNoSmoothing) timePitch.reset(); }
    else
    { smoothedTimeRatio = effectiveTime; smoothedPitchSemis = effectivePitchSemis; }

    timePitch.setTimeStretchRatio(smoothedTimeRatio);
    timePitch.setPitchSemitones(smoothedPitchSemis);

    // Branch engines cleanly: Naive vs RubberBand
    if (engine.load(std::memory_order_relaxed) == Engine::Naive)
    {
        auto* destL = buffer.getWritePointer(0);
        auto* destR = buffer.getNumChannels() > 1 ? buffer.getWritePointer(1) : destL;
        auto* srcL = sourceBuffer.getReadPointer(0);
        auto* srcR = sourceBuffer.getNumChannels() > 1 ? sourceBuffer.getReadPointer(1) : srcL;
        const double pitchScale = std::pow(2.0, (double) effectivePitchSemis / 12.0);
        const double step = pitchScale / (double) juce::jmax(0.0001f, effectiveTime);
        const double effectiveEndSample = (endSamplePos < 0.0 || endSamplePos >= sourceLength) ? (double)sourceLength - 1 : endSamplePos;
        for (int i = 0; i < numDestSamples; ++i)
        {
            int base = (int) readPosition;
            if (readPosition >= effectiveEndSample)
            {
                if (isLooping) 
                {
                    readPosition = startSamplePos + (readPosition - effectiveEndSample);
                    base = (int)readPosition;
                }
                else 
                { 
                    isPlaying = false;
                    buffer.clear(i, numDestSamples - i); 
                    break; 
                }
            }
            const int next = juce::jmin(sourceLength - 1, base + 1);
            const float frac = (float) (readPosition - (double) base);
            const float l = srcL[base] + frac * (srcL[next] - srcL[base]);
            const float r = srcR[base] + frac * (srcR[next] - srcR[base]);
            destL[i] = l; if (destR) destR[i] = r; readPosition += step;
        }
        return;
    }

    // Ensure interleaved buffers large enough
    if (numDestSamples > interleavedCapacityFrames)
    {
        interleavedCapacityFrames = numDestSamples;
        interleavedInput.allocate((size_t)(interleavedCapacityFrames * 2), true);
        interleavedOutput.allocate((size_t)(interleavedCapacityFrames * 2), true);
    }

    float* inLR = interleavedInput.getData();
    auto* srcL = sourceBuffer.getReadPointer(0);
    auto* srcR = sourceBuffer.getNumChannels() > 1 ? sourceBuffer.getReadPointer(1) : srcL;

    // RubberBand path: feed contiguous raw frames equal to output block size
    const double effectiveEndSample = (endSamplePos < 0.0 || endSamplePos >= sourceLength) ? (double)sourceLength - 1 : endSamplePos;
    int framesFed = 0;
    for (int i = 0; i < numDestSamples; ++i)
    {
        int pos = (int) readPosition;
        if (readPosition >= effectiveEndSample)
        {
            if (isLooping)
            {
                readPosition = startSamplePos + (readPosition - effectiveEndSample);
                pos = (int)readPosition;
            }
            else 
            {
                isPlaying = false;
                break;
            }
        }
        inLR[2 * i + 0] = srcL[pos];
        inLR[2 * i + 1] = srcR[pos];
        readPosition += 1.0;
        framesFed++;
    }

    if (framesFed > 0)
        timePitch.putInterleaved(inLR, framesFed);
    float* outLR = interleavedOutput.getData();
    int produced = timePitch.receiveInterleaved(outLR, numDestSamples);
    if (produced > 0)
    {
        auto* destL = buffer.getWritePointer(0);
        auto* destR = buffer.getNumChannels() > 1 ? buffer.getWritePointer(1) : destL;
        for (int i = 0; i < produced; ++i)
        { destL[i] = outLR[2 * i + 0]; if (destR) destR[i] = outLR[2 * i + 1]; }
        if (produced < numDestSamples)
        { buffer.clear(0, produced, numDestSamples - produced); if (destR) buffer.clear(1, produced, numDestSamples - produced); }
    }
}




================================================================================
FILE: juce\Source\audio\modules\TimeStretcherAudioSource.h
================================================================================


#pragma once

#include <juce_audio_basics/juce_audio_basics.h>
#include "../dsp/TimePitchProcessor.h"

/**
 * Wrapper around a PositionableAudioSource that applies time-stretching using RubberBand.
 * This allows audio playback speed to be synchronized with video playback speed.
 */
class TimeStretcherAudioSource : public juce::PositionableAudioSource
{
public:
    TimeStretcherAudioSource(juce::PositionableAudioSource* input, bool deleteInputWhenDeleted);
    ~TimeStretcherAudioSource() override;

    void prepareToPlay(int samplesPerBlockExpected, double sampleRate) override;
    void releaseResources() override;
    void getNextAudioBlock(const juce::AudioSourceChannelInfo& bufferToFill) override;

    void setNextReadPosition(juce::int64 newPosition) override;
    juce::int64 getNextReadPosition() const override;
    juce::int64 getTotalLength() const override;
    bool isLooping() const override;
    void setLooping(bool shouldLoop) override;

    // Custom method to control playback speed (0.25x to 4.0x)
    void setSpeed(double newSpeed);

private:
    juce::PositionableAudioSource* inputSource;
    bool deleteInput;

    TimePitchProcessor timePitch;
    double currentSpeed { 1.0 };
    bool isLooping_ { false };

    juce::AudioBuffer<float> inputBuffer;
    juce::AudioBuffer<float> stretchedBuffer;
    juce::AudioBuffer<float> interleavedInput;
    juce::AudioBuffer<float> interleavedOutput;

    bool isPrepared { false };
    bool isPrimed { false };
    double currentSampleRate { 44100.0 };
    int currentBlockSize { 512 };
};



================================================================================
FILE: juce\Source\audio\modules\TimeStretcherAudioSource.cpp
================================================================================


#include "TimeStretcherAudioSource.h"

TimeStretcherAudioSource::TimeStretcherAudioSource(juce::PositionableAudioSource* input, bool deleteInputWhenDeleted)
    : inputSource(input), deleteInput(deleteInputWhenDeleted)
{
    timePitch.setMode(TimePitchProcessor::Mode::RubberBand);
}

TimeStretcherAudioSource::~TimeStretcherAudioSource()
{
    releaseResources();
    if (deleteInput && inputSource != nullptr)
        delete inputSource;
}

void TimeStretcherAudioSource::prepareToPlay(int samplesPerBlockExpected, double sampleRate)
{
    currentSampleRate = sampleRate;
    currentBlockSize = samplesPerBlockExpected;

    if (inputSource != nullptr)
        inputSource->prepareToPlay(samplesPerBlockExpected, sampleRate);

    // Get number of channels from the reader source (typically stereo for video audio)
    int numChannels = 2; // Default to stereo
    if (inputSource != nullptr && inputSource->getTotalLength() > 0)
    {
        // Try to determine channel count - for AudioFormatReaderSource, we'll use 2 as default
        // since video audio is typically stereo
        numChannels = 2;
    }
    timePitch.prepare(sampleRate, numChannels, samplesPerBlockExpected);
    timePitch.reset();

    inputBuffer.setSize(numChannels, samplesPerBlockExpected * 2);
    stretchedBuffer.setSize(numChannels, samplesPerBlockExpected * 2);
    interleavedInput.setSize(1, samplesPerBlockExpected * numChannels * 2);
    interleavedOutput.setSize(1, samplesPerBlockExpected * numChannels * 2);

    isPrepared = true;
    isPrimed = false;
}

void TimeStretcherAudioSource::releaseResources()
{
    isPrepared = false;
    isPrimed = false;
    if (inputSource != nullptr)
        inputSource->releaseResources();
    timePitch.reset();
}

void TimeStretcherAudioSource::getNextAudioBlock(const juce::AudioSourceChannelInfo& bufferToFill)
{
    if (!isPrepared || inputSource == nullptr || bufferToFill.buffer == nullptr)
    {
        bufferToFill.clearActiveBufferRegion();
        return;
    }

    const int numSamples = bufferToFill.numSamples;
    const int numChannels = bufferToFill.buffer->getNumChannels();
    
    // Prime the stretcher if needed (RubberBand needs initial frames)
    if (!isPrimed && timePitch.availableFrames() < numSamples)
    {
        inputBuffer.setSize(numChannels, numSamples * 4, false, false, true);
        juce::AudioSourceChannelInfo primeInfo(inputBuffer);
        inputSource->getNextAudioBlock(primeInfo);
        
        // Convert to interleaved and feed to stretcher
        interleavedInput.setSize(1, primeInfo.numSamples * numChannels, false, false, true);
        float* interleaved = interleavedInput.getWritePointer(0);
        for (int i = 0; i < primeInfo.numSamples; ++i)
        {
            for (int ch = 0; ch < numChannels; ++ch)
            {
                interleaved[i * numChannels + ch] = inputBuffer.getSample(ch, i);
            }
        }
        timePitch.putInterleaved(interleaved, primeInfo.numSamples);
        isPrimed = true;
    }

    // Read from input source
    inputBuffer.setSize(numChannels, numSamples, false, false, true);
    juce::AudioSourceChannelInfo inputInfo(inputBuffer);
    inputSource->getNextAudioBlock(inputInfo);

    if (inputInfo.numSamples <= 0)
    {
        bufferToFill.clearActiveBufferRegion();
        return;
    }

    // Convert to interleaved
    interleavedInput.setSize(1, inputInfo.numSamples * numChannels, false, false, true);
    float* interleaved = interleavedInput.getWritePointer(0);
    for (int i = 0; i < inputInfo.numSamples; ++i)
    {
        for (int ch = 0; ch < numChannels; ++ch)
        {
            interleaved[i * numChannels + ch] = inputBuffer.getSample(ch, i);
        }
    }

    // Feed to time stretcher
    timePitch.putInterleaved(interleaved, inputInfo.numSamples);

    // Retrieve stretched audio
    interleavedOutput.setSize(1, numSamples * numChannels, false, false, true);
    float* stretchedInterleaved = interleavedOutput.getWritePointer(0);
    int framesReceived = timePitch.receiveInterleaved(stretchedInterleaved, numSamples);

    // Convert back to planar and fill output
    if (framesReceived > 0)
    {
        const int actualFrames = juce::jmin(framesReceived, numSamples);
        for (int ch = 0; ch < numChannels; ++ch)
        {
            float* output = bufferToFill.buffer->getWritePointer(ch, bufferToFill.startSample);
            for (int i = 0; i < actualFrames; ++i)
            {
                output[i] = stretchedInterleaved[i * numChannels + ch];
            }
            // Zero any remaining samples
            if (actualFrames < numSamples)
            {
                juce::FloatVectorOperations::clear(output + actualFrames, numSamples - actualFrames);
            }
        }
    }
    else
    {
        bufferToFill.clearActiveBufferRegion();
    }
}

void TimeStretcherAudioSource::setNextReadPosition(juce::int64 newPosition)
{
    if (inputSource != nullptr)
    {
        inputSource->setNextReadPosition(newPosition);
        timePitch.reset(); // Reset stretcher on seek
        isPrimed = false; // Need to re-prime after seek
    }
}

juce::int64 TimeStretcherAudioSource::getNextReadPosition() const
{
    return inputSource != nullptr ? inputSource->getNextReadPosition() : 0;
}

juce::int64 TimeStretcherAudioSource::getTotalLength() const
{
    return inputSource != nullptr ? inputSource->getTotalLength() : 0;
}

bool TimeStretcherAudioSource::isLooping() const
{
    return isLooping_;
}

void TimeStretcherAudioSource::setLooping(bool shouldLoop)
{
    isLooping_ = shouldLoop;
    if (inputSource != nullptr)
        inputSource->setLooping(shouldLoop);
}

void TimeStretcherAudioSource::setSpeed(double newSpeed)
{
    currentSpeed = juce::jlimit(0.25, 4.0, newSpeed);
    timePitch.setTimeStretchRatio(currentSpeed);
}



================================================================================
FILE: juce\Source\audio\modules\TimePitchModuleProcessor.h
================================================================================


#pragma once

#include "ModuleProcessor.h"
#include <juce_core/juce_core.h>
#include <juce_dsp/juce_dsp.h>
#include "../dsp/TimePitchProcessor.h"

class TimePitchModuleProcessor : public ModuleProcessor
{
public:
    // Parameter IDs
    static constexpr const char* paramIdSpeed     = "speed";
    static constexpr const char* paramIdPitch     = "pitch";
    static constexpr const char* paramIdEngine    = "engine";
    static constexpr const char* paramIdSpeedMod  = "speed_mod";
    static constexpr const char* paramIdPitchMod  = "pitch_mod";

    TimePitchModuleProcessor();
    ~TimePitchModuleProcessor() override = default;

    const juce::String getName() const override { return "timepitch"; }

    void prepareToPlay (double sampleRate, int samplesPerBlock) override;
    void releaseResources() override {}
    void processBlock (juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    juce::AudioProcessorValueTreeState& getAPVTS() override { return apvts; }
    
    juce::String getAudioInputLabel(int channel) const override
    {
        switch (channel)
        {
            case 0: return "In L";
            case 1: return "In R";
            case 2: return "Speed Mod";
            case 3: return "Pitch Mod";
            default: return juce::String("In ") + juce::String(channel + 1);
        }
    }
    
    // Parameter bus contract implementation
    bool getParamRouting(const juce::String& paramId, int& outBusIndex, int& outChannelIndexInBus) const override;
    
    std::vector<DynamicPinInfo> getDynamicInputPins() const override;
    std::vector<DynamicPinInfo> getDynamicOutputPins() const override;

#if defined(PRESET_CREATOR_UI)
    void drawParametersInNode (float itemWidth,
                               const std::function<bool(const juce::String& paramId)>& isParamModulated,
                               const std::function<void()>& onModificationEnded) override;
    void drawIoPins (const NodePinHelpers& helpers) override;
#endif

private:
    static juce::AudioProcessorValueTreeState::ParameterLayout createParameterLayout();

    juce::AudioProcessorValueTreeState apvts;

    TimePitchProcessor timePitch;
    juce::HeapBlock<float> interleavedInput;
    juce::HeapBlock<float> interleavedOutput;
    int interleavedCapacityFrames { 0 };

    // Parameter pointers
    std::atomic<float>* speedParam { nullptr };
    std::atomic<float>* pitchParam { nullptr };
    std::atomic<float>* speedModParam { nullptr };
    std::atomic<float>* pitchModParam { nullptr };
    juce::AudioParameterChoice* engineParam { nullptr };
    double sr { 48000.0 };

    // --- Streaming FIFO for live input buffering ---
    juce::AudioBuffer<float> inputFifo; // stereo FIFO storage
    juce::AbstractFifo abstractFifo { 0 }; // manages read/write indices
    int fifoSize { 0 };
    
    // Smoothed parameters for zipper-free modulation
    juce::SmoothedValue<float> speedSm;
    juce::SmoothedValue<float> pitchSm;
};




================================================================================
FILE: juce\Source\audio\modules\TimePitchModuleProcessor.cpp
================================================================================


#include "TimePitchModuleProcessor.h"

static inline void ensureCapacity (juce::HeapBlock<float>& block, int frames, int channels, int& capacityFrames)
{
    if (frames > capacityFrames)
    {
        capacityFrames = juce::jmax (frames, capacityFrames * 2 + 128);
        block.allocate ((size_t) (capacityFrames * channels), true);
    }
}

juce::AudioProcessorValueTreeState::ParameterLayout TimePitchModuleProcessor::createParameterLayout()
{
    std::vector<std::unique_ptr<juce::RangedAudioParameter>> p;
    p.push_back (std::make_unique<juce::AudioParameterFloat> (paramIdSpeed, "Speed", juce::NormalisableRange<float> (0.25f, 4.0f, 0.0001f, 0.5f), 1.0f));
    p.push_back (std::make_unique<juce::AudioParameterFloat> (paramIdPitch, "Pitch (st)", juce::NormalisableRange<float> (-24.0f, 24.0f, 0.01f), 0.0f));
    p.push_back (std::make_unique<juce::AudioParameterChoice> (paramIdEngine, "Engine", juce::StringArray { "RubberBand", "Naive" }, 0));
    p.push_back (std::make_unique<juce::AudioParameterFloat> (paramIdSpeedMod, "Speed Mod", juce::NormalisableRange<float> (0.25f, 4.0f, 0.0001f, 0.5f), 1.0f));
    p.push_back (std::make_unique<juce::AudioParameterFloat> (paramIdPitchMod, "Pitch Mod", juce::NormalisableRange<float> (-24.0f, 24.0f, 0.01f), 0.0f));
    return { p.begin(), p.end() };
}

TimePitchModuleProcessor::TimePitchModuleProcessor()
    : ModuleProcessor (BusesProperties()
        .withInput ("Inputs", juce::AudioChannelSet::discreteChannels(4), true) // ch0 L in, ch1 R in, ch2 Speed Mod, ch3 Pitch Mod
        .withOutput("Out", juce::AudioChannelSet::stereo(), true)),
      apvts (*this, nullptr, "TimePitchParams", createParameterLayout())
{
    speedParam     = apvts.getRawParameterValue (paramIdSpeed);
    pitchParam     = apvts.getRawParameterValue (paramIdPitch);
    speedModParam  = apvts.getRawParameterValue (paramIdSpeedMod);
    pitchModParam  = apvts.getRawParameterValue (paramIdPitchMod);
    engineParam    = dynamic_cast<juce::AudioParameterChoice*>(apvts.getParameter (paramIdEngine));

    lastOutputValues.clear();
    lastOutputValues.push_back (std::make_unique<std::atomic<float>> (0.0f));
    lastOutputValues.push_back (std::make_unique<std::atomic<float>> (0.0f));
    
    // Initialize smoothed values
    speedSm.reset(1.0f);
    pitchSm.reset(0.0f);
}

void TimePitchModuleProcessor::prepareToPlay (double sampleRate, int samplesPerBlock)
{
    sr = sampleRate;
    timePitch.prepare (sampleRate, 2, samplesPerBlock);

    // Initialize FIFO to ~2 seconds of audio
    fifoSize = (int) (sampleRate * 2.0);
    if (fifoSize < samplesPerBlock * 4) fifoSize = samplesPerBlock * 4; // safety minimum
    inputFifo.setSize (2, fifoSize);
    abstractFifo.setTotalSize (fifoSize);

    interleavedCapacityFrames = 0;
    ensureCapacity (interleavedInput, samplesPerBlock, 2, interleavedCapacityFrames);
    ensureCapacity (interleavedOutput, samplesPerBlock * 2, 2, interleavedCapacityFrames); // some headroom
    timePitch.reset();
}

void TimePitchModuleProcessor::processBlock (juce::AudioBuffer<float>& buffer, juce::MidiBuffer& midi)
{
    juce::ignoreUnused (midi);
    auto inBus = getBusBuffer(buffer, true, 0);  // Single bus
    auto outBus = getBusBuffer(buffer, false, 0);
    
    const int numSamples = buffer.getNumSamples();
    if (numSamples <= 0) return;

    // 1) Write incoming audio into FIFO (use inBus explicitly)
    int start1=0,size1=0,start2=0,size2=0;
    abstractFifo.prepareToWrite (numSamples, start1, size1, start2, size2);
    if (size1 > 0)
    {
        inputFifo.copyFrom (0, start1, inBus, 0, 0, size1);
        inputFifo.copyFrom (1, start1, inBus, 1, 0, size1);
    }
    if (size2 > 0)
    {
        inputFifo.copyFrom (0, start2, inBus, 0, size1, size2);
        inputFifo.copyFrom (1, start2, inBus, 1, size1, size2);
    }
    const int written = size1 + size2;
    abstractFifo.finishedWrite (written);

    // 2) Read params and configure engine
    const int engineIdx = engineParam != nullptr ? engineParam->getIndex() : 0;
    {
        static int currentMode = -1;
        const int requestedMode = (engineIdx == 0 ? (int) TimePitchProcessor::Mode::RubberBand : (int) TimePitchProcessor::Mode::Fifo);
        if (requestedMode != currentMode)
        {
            timePitch.reset();
            currentMode = requestedMode;
        }
        timePitch.setMode ((TimePitchProcessor::Mode) requestedMode);
    }

    // Get pointers to modulation CV inputs
    const bool isSpeedMod = isParamInputConnected("speed");
    const bool isPitchMod = isParamInputConnected("pitch");
    
    const float* speedCV = isSpeedMod && inBus.getNumChannels() > 2 ? inBus.getReadPointer(2) : nullptr;
    const float* pitchCV = isPitchMod && inBus.getNumChannels() > 3 ? inBus.getReadPointer(3) : nullptr;
    
    // Process in slices to reduce engine reconfig cost
    const int sliceSize = 32;
    for (int sliceStart = 0; sliceStart < numSamples; sliceStart += sliceSize)
    {
        const int sliceEnd = juce::jmin(sliceStart + sliceSize, numSamples);
        const int sliceSamples = sliceEnd - sliceStart;
        
        // Calculate target values from CV (use middle of slice)
        const int midSample = sliceStart + sliceSamples / 2;
        
        float targetSpeed = speedParam->load();
        if (isSpeedMod && speedCV != nullptr) {
            const float cv = juce::jlimit(0.0f, 1.0f, speedCV[midSample]);
            const float minSpeed = 0.25f;
            const float maxSpeed = 4.0f;
            targetSpeed = minSpeed * std::pow(maxSpeed / minSpeed, cv);
        }
        
        float targetPitch = pitchParam->load();
        if (isPitchMod && pitchCV != nullptr) {
            const float cv = juce::jlimit(0.0f, 1.0f, pitchCV[midSample]);
            targetPitch = -24.0f + cv * 48.0f; // -24 to +24 semitones
        }
        
        // Set targets and advance smoothing
        speedSm.setTargetValue(juce::jlimit(0.1f, 4.0f, targetSpeed));
        pitchSm.setTargetValue(juce::jlimit(-24.0f, 24.0f, targetPitch));
        
        // Update telemetry for live UI feedback (once per slice)
        setLiveParamValue("speed_live", speedSm.getCurrentValue());
        setLiveParamValue("pitch_live", pitchSm.getCurrentValue());
        
        // Advance smoothing for this slice
        for (int i = 0; i < sliceSamples; ++i) {
            speedSm.skip(1);
            pitchSm.skip(1);
        }
        
        // Configure engine with current smoothed values
        timePitch.setTimeStretchRatio(speedSm.getCurrentValue());
        timePitch.setPitchSemitones(pitchSm.getCurrentValue());
        
        // Publish telemetry
        setLiveParamValue("speed", speedSm.getCurrentValue());
        setLiveParamValue("pitch", pitchSm.getCurrentValue());
    }

    // 3) Compute frames needed to fill this block
    const double safeSpeed = juce::jlimit (0.1, 4.0, (double) speedSm.getCurrentValue());
    const int framesToFeed = juce::jmax (1, (int) std::ceil ((double) numSamples / safeSpeed));

    outBus.clear();
    if (abstractFifo.getNumReady() >= framesToFeed)
    {
        // 4) Read from FIFO and interleave
        ensureCapacity (interleavedInput, framesToFeed, 2, interleavedCapacityFrames);
        abstractFifo.prepareToRead (framesToFeed, start1, size1, start2, size2);
        auto* inL = inputFifo.getReadPointer (0);
        auto* inR = inputFifo.getReadPointer (1);
        float* inLR = interleavedInput.getData();
        for (int i = 0; i < size1; ++i) { inLR[2*i+0] = inL[start1 + i]; inLR[2*i+1] = inR[start1 + i]; }
        if (size2 > 0)
            for (int i = 0; i < size2; ++i) { inLR[2*(size1+i)+0] = inL[start2 + i]; inLR[2*(size1+i)+1] = inR[start2 + i]; }
        const int readCount = size1 + size2;
        abstractFifo.finishedRead (readCount);

        // 5) Process and copy back
        // Guard against engine internal errors with try/catch (non-RT critical path)
        try { timePitch.putInterleaved (inLR, framesToFeed); }
        catch (...) { /* swallow to avoid crash; output will be silence */ }
        ensureCapacity (interleavedOutput, numSamples, 2, interleavedCapacityFrames);
        int produced = 0;
        try { produced = timePitch.receiveInterleaved (interleavedOutput.getData(), numSamples); }
        catch (...) { produced = 0; }
        if (produced > 0)
        {
            const int outFrames = juce::jmin (numSamples, produced);
            const float* outLR = interleavedOutput.getData();
            float* L = outBus.getNumChannels() > 0 ? outBus.getWritePointer (0) : buffer.getWritePointer(0);
            float* R = outBus.getNumChannels() > 1 ? outBus.getWritePointer (1) : L;
            for (int i = 0; i < outFrames; ++i) { L[i] = outLR[2*i+0]; if (R) R[i] = outLR[2*i+1]; }
        }
    }

    // Update lastOutputValues
    if (lastOutputValues.size() >= 2)
    {
        lastOutputValues[0]->store (buffer.getMagnitude (0, 0, numSamples));
        lastOutputValues[1]->store (buffer.getNumChannels() > 1 ? buffer.getMagnitude (1, 0, numSamples) : 0.0f);
    }
}

// Parameter bus contract implementation
bool TimePitchModuleProcessor::getParamRouting(const juce::String& paramId, int& outBusIndex, int& outChannelIndexInBus) const
{
    outBusIndex = 0;
    if (paramId == "speed") { outChannelIndexInBus = 2; return true; }  // Speed Mod
    if (paramId == "pitch") { outChannelIndexInBus = 3; return true; }  // Pitch Mod
    return false;
}

#if defined(PRESET_CREATOR_UI)
void TimePitchModuleProcessor::drawParametersInNode (float itemWidth,
                                                    const std::function<bool(const juce::String& paramId)>& isParamModulated,
                                                    const std::function<void()>& onModificationEnded)
{
    ImGui::PushItemWidth (itemWidth);
    auto& ap = getAPVTS();

    // Speed
    bool spMod = isParamModulated ("speed");
    if (spMod) { 
        ImGui::BeginDisabled(); 
        ImGui::PushStyleColor (ImGuiCol_FrameBg, ImVec4 (1,1,0,0.3f)); 
    }
    float speed = ap.getRawParameterValue (paramIdSpeed)->load();
    if (spMod) {
        speed = getLiveParamValueFor("speed", "speed_live", speed);
    }
    if (ImGui::SliderFloat ("Speed", &speed, 0.25f, 4.0f, "%.2fx")) {
        if (!spMod) {
            if (auto* p = dynamic_cast<juce::AudioParameterFloat*>(ap.getParameter (paramIdSpeed))) *p = speed;
        }
    }
    if (!spMod) adjustParamOnWheel (ap.getParameter (paramIdSpeed), paramIdSpeed, speed);
    if (ImGui::IsItemDeactivatedAfterEdit()) onModificationEnded();
    if (spMod) { ImGui::PopStyleColor(); ImGui::EndDisabled(); }

    // Pitch
    bool piMod = isParamModulated ("pitch");
    if (piMod) { 
        ImGui::BeginDisabled(); 
        ImGui::PushStyleColor (ImGuiCol_FrameBg, ImVec4 (1,1,0,0.3f)); 
    }
    float pitch = ap.getRawParameterValue (paramIdPitch)->load();
    if (piMod) {
        pitch = getLiveParamValueFor("pitch", "pitch_live", pitch);
    }
    if (ImGui::SliderFloat ("Pitch", &pitch, -24.0f, 24.0f, "%.1f st"))
        if (!piMod) if (auto* p = dynamic_cast<juce::AudioParameterFloat*>(ap.getParameter (paramIdPitch))) *p = pitch;
    if (!piMod) adjustParamOnWheel (ap.getParameter (paramIdPitch), paramIdPitch, pitch);
    if (ImGui::IsItemDeactivatedAfterEdit()) onModificationEnded();
    if (piMod) { ImGui::PopStyleColor(); ImGui::EndDisabled(); }

    // Engine
    int engineIdx = engineParam != nullptr ? engineParam->getIndex() : 0;
    const char* items[] = { "RubberBand", "Naive" };
    if (ImGui::Combo ("Engine", &engineIdx, items, 2))
        if (engineParam) *engineParam = engineIdx;
    if (ImGui::IsItemDeactivatedAfterEdit()) onModificationEnded();

    ImGui::PopItemWidth();
}

void TimePitchModuleProcessor::drawIoPins (const NodePinHelpers& helpers)
{
    helpers.drawAudioInputPin ("In L", 0);
    helpers.drawAudioInputPin ("In R", 1);
    helpers.drawAudioInputPin ("Speed Mod", 2);
    helpers.drawAudioInputPin ("Pitch Mod", 3);

    helpers.drawAudioOutputPin ("Out L", 0);
    helpers.drawAudioOutputPin ("Out R", 1);
}
#endif

std::vector<DynamicPinInfo> TimePitchModuleProcessor::getDynamicInputPins() const
{
    std::vector<DynamicPinInfo> pins;
    
    // Audio inputs (channels 0-1)
    pins.push_back({"In L", 0, PinDataType::Audio});
    pins.push_back({"In R", 1, PinDataType::Audio});
    
    // Modulation inputs (channels 2-3)
    pins.push_back({"Speed Mod", 2, PinDataType::CV});
    pins.push_back({"Pitch Mod", 3, PinDataType::CV});
    
    return pins;
}

std::vector<DynamicPinInfo> TimePitchModuleProcessor::getDynamicOutputPins() const
{
    std::vector<DynamicPinInfo> pins;
    
    // Audio outputs (channels 0-1)
    pins.push_back({"Out L", 0, PinDataType::Audio});
    pins.push_back({"Out R", 1, PinDataType::Audio});
    
    return pins;
}




================================================================================
FILE: juce\Source\audio\modules\VideoFileLoaderModule.h
================================================================================


#pragma once

#include "ModuleProcessor.h"
#include "FFmpegAudioReader.h"
#include "../dsp/TimePitchProcessor.h"
#include <opencv2/core.hpp>
#include <opencv2/videoio.hpp>
#include <juce_core/juce_core.h>
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_audio_formats/juce_audio_formats.h>
#include <juce_audio_basics/juce_audio_basics.h>
#include <juce_graphics/juce_graphics.h>

/**
 * Source node that loads a video file and publishes frames to VideoFrameManager.
 * Outputs its own logical ID as a CV signal for routing to processing nodes.
 */
class VideoFileLoaderModule : public ModuleProcessor, private juce::Thread
{
public:
    VideoFileLoaderModule();
    ~VideoFileLoaderModule() override;

    const juce::String getName() const override { return "video_file_loader"; }
    
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer& midi) override;
    void setTimingInfo(const TransportState& state) override { lastTransportPlaying.store(state.isPlaying); if (syncToTransport.load()) playing.store(state.isPlaying); }
    
    juce::AudioProcessorValueTreeState& getAPVTS() override { return apvts; }
    
    // Dynamic pin definitions
    std::vector<DynamicPinInfo> getDynamicOutputPins() const override;
    
    // State management for saving/loading video file path
    juce::ValueTree getExtraStateTree() const override;
    void setExtraStateTree(const juce::ValueTree& state) override;

    // For UI
    juce::Image getLatestFrame();
    void chooseVideoFile();

#if defined(PRESET_CREATOR_UI)
    void drawParametersInNode(float itemWidth,
                              const std::function<bool(const juce::String& paramId)>& isParamModulated,
                              const std::function<void()>& onModificationEnded) override;
    void drawIoPins(const NodePinHelpers& helpers) override;
    
    // Override to specify custom node width. Height is calculated dynamically based on video aspect ratio.
    // Width changes based on zoom level (Small=240px, Normal=480px, Large=960px).
    ImVec2 getCustomNodeSize() const override;
#endif

private:
    void run() override;
    void updateGuiFrame(const cv::Mat& frame);
    
    static juce::AudioProcessorValueTreeState::ParameterLayout createParameterLayout();
    juce::AudioProcessorValueTreeState apvts;
    
    std::atomic<float>* loopParam = nullptr;
    // 0 = Small (240), 1 = Normal (480), 2 = Large (960)
    std::atomic<float>* zoomLevelParam = nullptr;
    // Playback controls
    std::atomic<float>* speedParam = nullptr;   // 0.25 .. 4.0 (1.0 default)
    std::atomic<float>* inNormParam = nullptr;  // 0..1
    std::atomic<float>* outNormParam = nullptr; // 0..1
    std::atomic<float>* syncParam = nullptr; // bool as float
    juce::AudioParameterChoice* engineParam = nullptr;
    
    std::atomic<bool> playing { true };
    std::atomic<bool> syncToTransport { true };
    std::atomic<bool> lastTransportPlaying { false };
    std::atomic<bool> needPreviewFrame { false };
    std::atomic<bool> lastPlaying { false }; // for play-edge detection
    std::atomic<int> lastFourcc { 0 }; // cached FOURCC
    std::atomic<int> pendingSeekFrame { -1 };
    std::atomic<int> lastPosFrame { 0 };
    std::atomic<double> totalDurationMs { 0.0 };
    
    // Unified, thread-safe seeking mechanism for both video and audio
    std::atomic<float> pendingSeekNormalized { -1.0f };
    
    cv::VideoCapture videoCapture;
    juce::CriticalSection captureLock;

    static juce::String fourccToString(int fourcc)
    {
        if (fourcc == 0) return "unknown";
        char c[5];
        c[0] = (char)(fourcc & 0xFF);
        c[1] = (char)((fourcc >> 8) & 0xFF);
        c[2] = (char)((fourcc >> 16) & 0xFF);
        c[3] = (char)((fourcc >> 24) & 0xFF);
        c[4] = '\0';
        return juce::String(c);
    }

    static juce::String fourccFriendlyName(const juce::String& code)
    {
        const juce::String c = code.toLowerCase();
        if (c == "avc1" || c == "h264") return "H.264";
        if (c == "hvc1" || c == "hevc" || c == "hev1") return "H.265/HEVC";
        if (c == "mp4v" || c == "m4v") return "MPEG-4 Part 2";
        if (c == "mjpg" || c == "mjpa" || c == "jpeg") return "Motion JPEG";
        if (c == "xvid" ) return "MPEG-4 ASP (Xvid)";
        if (c == "vp09") return "VP9";
        if (c == "av01") return "AV1";
        if (c == "wmv3" || c == "wvc1") return "VC-1";
        if (c == "h263") return "H.263";
        return "unknown";
    }
    juce::Image latestFrameForGui;
    juce::CriticalSection imageLock;
    
    juce::File videoFileToLoad;
    juce::File currentVideoFile;
    std::unique_ptr<juce::FileChooser> fileChooser;

    // Cached metadata (atomic for cross-thread visibility)
    std::atomic<int> totalFrames { 0 };
    
    // Audio playback engine
    std::unique_ptr<FFmpegAudioReader> audioReader;
    TimePitchProcessor timePitch;
    double audioReadPosition = 0.0;
    double audioSampleRate = 44100.0;
    std::atomic<bool> audioLoaded { false };
    juce::CriticalSection audioLock;
    
    // Buffers for audio processing
    juce::HeapBlock<float> interleavedInput;
    juce::HeapBlock<float> interleavedOutput;
    int interleavedCapacityFrames = 0;
    
    void loadAudioFromVideo();
};



================================================================================
FILE: juce\Source\audio\modules\VideoFileLoaderModule.cpp
================================================================================


#include "VideoFileLoaderModule.h"
#include "../../video/VideoFrameManager.h"
#include <opencv2/imgproc.hpp>

#if defined(PRESET_CREATOR_UI)
#include <imgui.h>
#endif

juce::AudioProcessorValueTreeState::ParameterLayout VideoFileLoaderModule::createParameterLayout()
{
    std::vector<std::unique_ptr<juce::RangedAudioParameter>> params;
    
    params.push_back(std::make_unique<juce::AudioParameterBool>("loop", "Loop", true));
    params.push_back(std::make_unique<juce::AudioParameterBool>("sync", "Sync to Transport", true));
    params.push_back(std::make_unique<juce::AudioParameterChoice>(
        "zoomLevel", "Zoom Level", juce::StringArray{ "Small", "Normal", "Large" }, 1));
    // Playback controls
    params.push_back(std::make_unique<juce::AudioParameterFloat>(
        "speed", "Speed", juce::NormalisableRange<float>(0.25f, 4.0f, 0.01f), 1.0f));
    params.push_back(std::make_unique<juce::AudioParameterFloat>(
        "in", "Start", 0.0f, 1.0f, 0.0f));
    params.push_back(std::make_unique<juce::AudioParameterFloat>(
        "out", "End", 0.0f, 1.0f, 1.0f));
    
    // Add the engine selection parameter, defaulting to Naive for performance
    params.push_back(std::make_unique<juce::AudioParameterChoice>("engine", "Engine", juce::StringArray { "RubberBand", "Naive" }, 1));
    
    return { params.begin(), params.end() };
}

VideoFileLoaderModule::VideoFileLoaderModule()
    : ModuleProcessor(BusesProperties()
                     .withOutput("CV Out", juce::AudioChannelSet::mono(), true)
                     .withOutput("Audio Out", juce::AudioChannelSet::stereo(), true)),
      juce::Thread("Video File Loader Thread"),
      apvts(*this, nullptr, "VideoFileLoaderParams", createParameterLayout())
{
    loopParam = apvts.getRawParameterValue("loop");
    zoomLevelParam = apvts.getRawParameterValue("zoomLevel");
    speedParam = apvts.getRawParameterValue("speed");
    inNormParam = apvts.getRawParameterValue("in");
    outNormParam = apvts.getRawParameterValue("out");
    syncParam = apvts.getRawParameterValue("sync");
    engineParam = dynamic_cast<juce::AudioParameterChoice*>(apvts.getParameter("engine"));
    syncToTransport.store(syncParam && (*syncParam > 0.5f));
}

VideoFileLoaderModule::~VideoFileLoaderModule()
{
    stopThread(5000);
    VideoFrameManager::getInstance().removeSource(getLogicalId());
}

void VideoFileLoaderModule::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    const juce::ScopedLock lock(audioLock);
    audioSampleRate = sampleRate;
    timePitch.prepare(sampleRate, 2, samplesPerBlock); // Prepare for stereo
    
    // Allocate interleaved buffers for processing
    interleavedCapacityFrames = samplesPerBlock * 2; // Headroom
    interleavedInput.allocate((size_t)(interleavedCapacityFrames * 2), true);
    interleavedOutput.allocate((size_t)(interleavedCapacityFrames * 2), true);
    
    startThread(juce::Thread::Priority::normal);
    // If we already had a file open previously, request re-open on thread start
    if (currentVideoFile.existsAsFile())
        videoFileToLoad = currentVideoFile;
}

void VideoFileLoaderModule::releaseResources()
{
    signalThreadShouldExit();
    stopThread(5000);
    
    const juce::ScopedLock lock(audioLock);
    timePitch.reset();
}

void VideoFileLoaderModule::chooseVideoFile()
{
    fileChooser = std::make_unique<juce::FileChooser>("Select a video file...", 
                                                       juce::File{}, 
                                                       "*.mp4;*.mov;*.avi;*.mkv;*.wmv");
    auto chooserFlags = juce::FileBrowserComponent::openMode | juce::FileBrowserComponent::canSelectFiles;
    
    fileChooser->launchAsync(chooserFlags, [this](const juce::FileChooser& fc)
    {
        auto file = fc.getResult();
        if (file.existsAsFile())
        {
            videoFileToLoad = file;
        }
    });
}

void VideoFileLoaderModule::run()
{
    // One-time OpenCV build summary: detect if FFMPEG is integrated
    {
        static std::atomic<bool> buildInfoLogged { false };
        if (!buildInfoLogged.exchange(true))
        {
            juce::String info(cv::getBuildInformation().c_str());
            bool ffmpegYes = false;
            juce::String ffmpegLine;
            {
                juce::StringArray lines;
                lines.addLines(info);
                for (const auto& ln : lines)
                {
                    if (ln.containsIgnoreCase("FFMPEG:"))
                    {
                        ffmpegLine = ln.trim();
                        if (ln.containsIgnoreCase("YES")) ffmpegYes = true;
                        break;
                    }
                }
            }
            juce::Logger::writeToLog("[OpenCV Build] FFMPEG integrated: " + juce::String(ffmpegYes ? "YES" : "NO") +
                                     (ffmpegLine.isNotEmpty() ? juce::String(" | ") + ffmpegLine : juce::String()));
        }
    }

    bool sourceIsOpen = false;
    double videoFps = 30.0; // Default FPS
    double frameDurationMs = 33.0; // Default to ~30fps
    
    while (!threadShouldExit())
    {
        // Check if user requested a new file OR we need to (re)open the same file after restart
        if (videoFileToLoad.existsAsFile() && (!videoCapture.isOpened() || videoFileToLoad != currentVideoFile))
        {
            if (videoCapture.isOpened())
            {
                videoCapture.release();
            }
            
            bool opened = videoCapture.open(videoFileToLoad.getFullPathName().toStdString(), cv::CAP_FFMPEG);
            if (!opened)
            {
                juce::Logger::writeToLog("[VideoFileLoader] FFmpeg backend open failed, retrying default backend: " + videoFileToLoad.getFullPathName());
                opened = videoCapture.open(videoFileToLoad.getFullPathName().toStdString());
            }
            if (opened)
            {
                currentVideoFile = videoFileToLoad;
                videoFileToLoad = juce::File{}; // Clear request immediately after processing
                sourceIsOpen = true;
                needPreviewFrame.store(true);
                // Log backend name (helps diagnose FFmpeg vs MSMF at runtime)
               #if (CV_VERSION_MAJOR >= 4)
                juce::String backendName = videoCapture.getBackendName().c_str();
                juce::Logger::writeToLog("[VideoFileLoader] Backend: " + backendName);
               #endif
                // Reset state for new media, but keep user-defined in/out ranges
                totalFrames.store(0); // force re-evaluation
                lastPosFrame.store(0);
                pendingSeekFrame.store(0);
                
                // Get the video's native FPS and codec
                videoFps = videoCapture.get(cv::CAP_PROP_FPS);
                lastFourcc.store((int) videoCapture.get(cv::CAP_PROP_FOURCC));
                {
                    // Backend and raw FOURCC diagnostics
                    #if CV_VERSION_MAJOR >= 4
                    const std::string backendName = videoCapture.getBackendName();
                    juce::Logger::writeToLog("[VideoFileLoader] Backend: " + juce::String(backendName.c_str()));
                    #endif
                    const int fourccRaw = lastFourcc.load();
                    juce::Logger::writeToLog("[VideoFileLoader] Metadata: FPS=" + juce::String(videoFps,2) +
                                             ", Raw FOURCC=" + juce::String(fourccRaw) +
                                             " ('" + fourccToString(fourccRaw) + "')");
                }
                {
                    int tf = (int) videoCapture.get(cv::CAP_PROP_FRAME_COUNT);
                    if (tf <= 1) tf = 0; // treat unknown/invalid as 0 so UI uses normalized seeks
                    totalFrames.store(tf);
                    juce::Logger::writeToLog("[VideoFileLoader] Opened '" + currentVideoFile.getFileName() + "' frames=" + juce::String(tf) +
                                             ", fps=" + juce::String(videoFps,2) + ", fourcc='" + fourccToString(lastFourcc.load()) + "'");
                    if (tf > 1 && videoFps > 0.0)
                        totalDurationMs.store((double)tf * (1000.0 / videoFps));
                    else
                        totalDurationMs.store(0.0);
                }
                if (videoFps > 0.0 && videoFps < 1000.0) // Sanity check
                {
                    frameDurationMs = 1000.0 / videoFps;
                    juce::Logger::writeToLog("[VideoFileLoader] Opened: " + currentVideoFile.getFileName() + 
                                            " (FPS: " + juce::String(videoFps, 2) + ")");
                }
                else
                {
                    frameDurationMs = 33.0; // Fallback to 30fps
                    juce::Logger::writeToLog("[VideoFileLoader] Opened: " + currentVideoFile.getFileName() + 
                                            " (FPS unknown, using 30fps)");
                }
                
                // Load audio from the video file
                loadAudioFromVideo();
            }
            else
            {
                juce::Logger::writeToLog("[VideoFileLoader] Failed to open: " + videoFileToLoad.getFullPathName());
                videoFileToLoad = juce::File{};
            }
        }
        
        if (!sourceIsOpen)
        {
            wait(500);
            continue;
        }
        
        // --- UNIFIED SEEK LOGIC ---
        // This is now the single point of control for seeking, triggered by UI.
        float normSeekPos = pendingSeekNormalized.exchange(-1.0f);
        if (normSeekPos >= 0.0f)
        {
            const double durMs = totalDurationMs.load();
            if (durMs > 0.0)
            {
                double seekToMs = juce::jlimit(0.0, durMs, (double)normSeekPos * durMs);
                
                // Seek video
                const juce::ScopedLock capLock(captureLock);
                if (videoCapture.isOpened())
                    videoCapture.set(cv::CAP_PROP_POS_MSEC, seekToMs);
                
                // Seek audio by updating the read position
                const juce::ScopedLock audioLk(audioLock);
                if (audioReader) {
                    audioReadPosition = normSeekPos * audioReader->lengthInSamples;
                    timePitch.reset(); // Reset time stretcher state after seek
                }
            }
            // Fallback to ratio seek if duration is not known
            else
            {
                const juce::ScopedLock capLock(captureLock);
                if (videoCapture.isOpened())
                    videoCapture.set(cv::CAP_PROP_POS_AVI_RATIO, (double)normSeekPos);
                
                // Seek audio by ratio
                const juce::ScopedLock audioLk(audioLock);
                if (audioReader) {
                    audioReadPosition = normSeekPos * audioReader->lengthInSamples;
                    timePitch.reset();
                }
            }
            needPreviewFrame.store(true);
        }

        // Legacy frame-based seek support (for compatibility)
        int seekTo = pendingSeekFrame.exchange(-1);
        if (seekTo >= 0)
        {
            const juce::ScopedLock capLock(captureLock);
            if (videoCapture.isOpened())
                videoCapture.set(cv::CAP_PROP_POS_FRAMES, (double) seekTo);
            needPreviewFrame.store(true);
            
            // Also seek audio
            const juce::ScopedLock audioLk(audioLock);
            if (audioReader) {
                int tfLocal = totalFrames.load();
                if (tfLocal > 1) {
                    float normPos = (float)seekTo / (float)(tfLocal - 1);
                    audioReadPosition = normPos * audioReader->lengthInSamples;
                    timePitch.reset();
                }
            }
        }

        // On play edge: seek to IN point using unified seek
        bool nowPlaying = playing.load();
        bool wasPlaying = lastPlaying.exchange(nowPlaying);
        if (nowPlaying && !wasPlaying)
        {
            float inN = inNormParam ? inNormParam->load() : 0.0f;
            pendingSeekNormalized.store(inN); // Use unified seek
            needPreviewFrame.store(true);
            
            // Also seek audio
            const juce::ScopedLock audioLk(audioLock);
            if (audioReader) {
                audioReadPosition = inN * audioReader->lengthInSamples;
                timePitch.reset();
            }
        }

        // Respect play/pause
        if (!playing.load())
        {
            // If paused, show a single preview frame after (re)open
            if (needPreviewFrame.exchange(false))
            {
                cv::Mat preview;
                if (videoCapture.isOpened())
                {
                    videoCapture.read(preview);
                }
                {
                    if (!preview.empty())
                    {
                        VideoFrameManager::getInstance().setFrame(getLogicalId(), preview);
                        updateGuiFrame(preview);
                        juce::Logger::writeToLog("[VideoFileLoader][Preview] Published paused preview frame");
                        lastPosFrame.store((int) videoCapture.get(cv::CAP_PROP_POS_FRAMES));
                        if (lastFourcc.load() == 0)
                            lastFourcc.store((int) videoCapture.get(cv::CAP_PROP_FOURCC));
                        // Also try to refresh total frames after the first paused read
                        if (totalFrames.load() <= 1)
                        {
                            int tf = (int) videoCapture.get(cv::CAP_PROP_FRAME_COUNT);
                            if (tf > 1)
                            {
                                totalFrames.store(tf);
                                juce::Logger::writeToLog("[VideoFileLoader] Frame count acquired after paused read: " + juce::String(tf));
                            }
                        }
                    }
                }
            }
            wait(40);
            continue;
        }

        // Time the frame processing to maintain correct playback speed
        auto loopStartTime = juce::Time::getMillisecondCounterHiRes();
        
        cv::Mat frame;
        if (videoCapture.isOpened())
            videoCapture.read(frame);
        if (!frame.empty())
        {
            // Publish frame to central manager
            VideoFrameManager::getInstance().setFrame(getLogicalId(), frame);
            
            // Update local preview
            updateGuiFrame(frame);
            lastPosFrame.store((int) videoCapture.get(cv::CAP_PROP_POS_FRAMES));
            if (lastFourcc.load() == 0)
                lastFourcc.store((int) videoCapture.get(cv::CAP_PROP_FOURCC));
            // If frame count was unknown at open time, refresh it after first (or any) read
            if (totalFrames.load() <= 1)
            {
                int tf = (int) videoCapture.get(cv::CAP_PROP_FRAME_COUNT);
                if (tf > 1)
                {
                    totalFrames.store(tf);
                    if (videoFps > 0.0)
                        totalDurationMs.store((double)tf * (1000.0 / videoFps));
                    juce::Logger::writeToLog("[VideoFileLoader] Frame count acquired after playing read: " + juce::String(tf));
                }
            }

            // Trim window enforcement (only when frame count available)
            if (totalFrames.load() > 1)
            {
                int pos = 0;
                {
                    const juce::ScopedLock capLock(captureLock);
                    if (videoCapture.isOpened())
                        pos = (int) videoCapture.get(cv::CAP_PROP_POS_FRAMES);
                }
                float inN = inNormParam ? inNormParam->load() : 0.0f;
                float outN = outNormParam ? outNormParam->load() : 1.0f;
                inN = juce::jlimit(0.0f, 1.0f, inN);
                outN = juce::jlimit(0.0f, 1.0f, outN);
                if (outN <= inN) outN = juce::jmin(1.0f, inN + 0.01f);
                const int tfLocal = totalFrames.load();
                int inF = juce::jlimit(0, juce::jmax(0, tfLocal - 1), (int)std::round(inN * (tfLocal - 1)));
                int outF = juce::jlimit(inF + 1, juce::jmax(1, tfLocal), (int)std::round(outN * (tfLocal - 1)));
                if (pos < inF)
                {
                    juce::Logger::writeToLog("[VideoFileLoader][Trim] pos=" + juce::String(pos) + " < inF=" + juce::String(inF) + " -> seeking to inF");
                    const juce::ScopedLock capLock(captureLock);
                    if (videoCapture.isOpened())
                        videoCapture.set(cv::CAP_PROP_POS_FRAMES, (double)inF);
                }
                else if (pos >= outF)
                {
                    if (loopParam && loopParam->load() > 0.5f)
                    {
                        juce::Logger::writeToLog("[VideoFileLoader][Trim] pos>=outF looping to inF=" + juce::String(inF));
                        const juce::ScopedLock capLock(captureLock);
                        if (videoCapture.isOpened())
                            videoCapture.set(cv::CAP_PROP_POS_FRAMES, (double)inF);
                    }
                    else
                    {
                        juce::Logger::writeToLog("[VideoFileLoader][Trim] pos>=outF stopping playback");
                        playing.store(false);
                    }
                }
            }
        }
        else
        {
            // End of video file
            if (loopParam->load() > 0.5f && currentVideoFile.existsAsFile())
            {
                // Loop: reopen the file
                videoCapture.release();
                if (videoCapture.open(currentVideoFile.getFullPathName().toStdString()))
                {
                    sourceIsOpen = true;
                    juce::Logger::writeToLog("[VideoFileLoader] Looping: " + currentVideoFile.getFileName());
                }
                else
                {
                    sourceIsOpen = false;
                }
            }
            else
            {
                // Stop
                sourceIsOpen = false;
                videoCapture.release();
            }
        }
        
        // Calculate how long to wait to maintain the original FPS
        auto processingTime = juce::Time::getMillisecondCounterHiRes() - loopStartTime;
        float spd = speedParam ? speedParam->load() : 1.0f;
        spd = juce::jlimit(0.1f, 4.0f, spd);
        int waitTime = (int)((frameDurationMs / spd) - processingTime);
        if (waitTime > 0)
        {
            wait(waitTime);
        }
        // If processing took longer than frame duration, don't wait (play as fast as possible)
    }
    
    videoCapture.release();
    VideoFrameManager::getInstance().removeSource(getLogicalId());
}

void VideoFileLoaderModule::updateGuiFrame(const cv::Mat& frame)
{
    cv::Mat bgraFrame;
    cv::cvtColor(frame, bgraFrame, cv::COLOR_BGR2BGRA);
    
    const juce::ScopedLock lock(imageLock);
    
    if (latestFrameForGui.isNull() || 
        latestFrameForGui.getWidth() != bgraFrame.cols || 
        latestFrameForGui.getHeight() != bgraFrame.rows)
    {
        latestFrameForGui = juce::Image(juce::Image::ARGB, bgraFrame.cols, bgraFrame.rows, true);
    }
    
    juce::Image::BitmapData destData(latestFrameForGui, juce::Image::BitmapData::writeOnly);
    memcpy(destData.data, bgraFrame.data, bgraFrame.total() * bgraFrame.elemSize());
}

juce::Image VideoFileLoaderModule::getLatestFrame()
{
    const juce::ScopedLock lock(imageLock);
    return latestFrameForGui.createCopy();
}

void VideoFileLoaderModule::loadAudioFromVideo()
{
    const juce::ScopedLock lock(audioLock);
    
    audioReader.reset();
    timePitch.reset(); // Reset the processing engine
    audioReadPosition = 0.0;
    audioLoaded.store(false);
    
    if (!currentVideoFile.existsAsFile()) return;
    
    try
    {
        audioReader = std::make_unique<FFmpegAudioReader>(currentVideoFile.getFullPathName());
        
        if (audioReader != nullptr && audioReader->lengthInSamples > 0)
        {
            audioLoaded.store(true);
            juce::Logger::writeToLog("[VideoFileLoader] Audio loaded via FFmpeg.");
        }
        else
        {
            juce::Logger::writeToLog("[VideoFileLoader] Could not extract audio stream via FFmpeg.");
        }
    }
    catch (const std::exception& e)
    {
        juce::Logger::writeToLog("[VideoFileLoader] Exception loading audio: " + juce::String(e.what()));
    }
}

void VideoFileLoaderModule::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer& midi)
{
    juce::ignoreUnused(midi);
    
    auto cvOutBus = getBusBuffer(buffer, false, 0);
    auto audioOutBus = getBusBuffer(buffer, false, 1);
    
    cvOutBus.clear();
    audioOutBus.clear();
    
    // Output Source ID on CV bus
    if (cvOutBus.getNumChannels() > 0)
    {
        float sourceId = (float)getLogicalId();
        for (int sample = 0; sample < cvOutBus.getNumSamples(); ++sample)
        {
            cvOutBus.setSample(0, sample, sourceId);
        }
    }
    
    // --- NEW AUDIO PROCESSING LOGIC ---
    const juce::ScopedLock lock(audioLock);
    if (!audioLoaded.load() || !audioReader || !playing.load())
    {
        return; // Nothing to play
    }

    // 1. Get parameters and configure the time-stretching engine
    const float speed = juce::jlimit(0.25f, 4.0f, speedParam ? speedParam->load() : 1.0f);
    const int engineIdx = engineParam ? engineParam->getIndex() : 1; // Default to Naive
    auto requestedMode = (engineIdx == 0) ? TimePitchProcessor::Mode::RubberBand : TimePitchProcessor::Mode::Fifo;
    
    timePitch.setMode(requestedMode); // Set engine mode
    timePitch.setTimeStretchRatio(speed); // Set playback speed

    // 2. Determine playback range in samples
    const float startNorm = inNormParam ? inNormParam->load() : 0.0f;
    const float endNorm = outNormParam ? outNormParam->load() : 1.0f;
    const juce::int64 startSample = (juce::int64)(startNorm * audioReader->lengthInSamples);
    juce::int64 endSample = (juce::int64)(endNorm * audioReader->lengthInSamples);
    if (endSample <= startSample) endSample = audioReader->lengthInSamples; // Use full length if range is invalid
    
    const bool isLooping = loopParam && loopParam->load() > 0.5f;
    const int numSamples = audioOutBus.getNumSamples();
    const int numChannels = audioReader->numChannels;

    // 3. THE FIX: Read a BLOCK of audio efficiently - don't read sample-by-sample
    // Check if we need to loop before reading
    if (audioReadPosition >= endSample)
    {
        if (isLooping) {
            audioReadPosition = (double)startSample; // Loop back
        } else {
            playing.store(false); // Stop playback
            return;
        }
    }
    
    // Calculate how many source samples we need to read to produce numSamples of output at current speed
    const int framesToFeed = (int)std::ceil((double)numSamples / speed);
    
    // Clamp to available range
    juce::int64 currentPos = (juce::int64)audioReadPosition;
    currentPos = juce::jlimit((juce::int64)0, (juce::int64)(audioReader->lengthInSamples - 1), currentPos);
    int samplesToRead = juce::jmin(framesToFeed, (int)(endSample - audioReadPosition));
    if (samplesToRead <= 0) {
        // No samples available in range
        return;
    }
    
    // Allocate source buffer for reading a block
    juce::AudioBuffer<float> sourceBuffer(numChannels, samplesToRead);
    
    // Prepare channel pointers for readSamples - it expects int* const* but with
    // usesFloatingPointData=true, we cast to float* const*
    float* channelPointers[64]; // Support up to 64 channels
    for (int ch = 0; ch < numChannels; ++ch) {
        channelPointers[ch] = sourceBuffer.getWritePointer(ch);
    }
    
    // Cast to int* const* for readSamples (the function signature uses int* but
    // when usesFloatingPointData is true, it's actually float data)
    int* const* channelPointersInt = reinterpret_cast<int* const*>(channelPointers);
    
    // Read a contiguous block of samples from the FFmpeg reader in one efficient operation
    if (!audioReader->readSamples(channelPointersInt, numChannels, 0, currentPos, samplesToRead))
    {
        // Failed to read, stop playback
        playing.store(false);
        return;
    }
    
    // Advance the read position by the block size
    audioReadPosition += samplesToRead;

    // 4. Convert planar to interleaved and feed to time stretcher
    // Ensure our interleaved buffers are large enough
    if (samplesToRead > interleavedCapacityFrames)
    {
        interleavedCapacityFrames = samplesToRead * 2;
        interleavedInput.allocate((size_t)(interleavedCapacityFrames * numChannels), true);
        interleavedOutput.allocate((size_t)(interleavedCapacityFrames * numChannels), true);
    }
    
    float* inLR = interleavedInput.getData();
    for (int i = 0; i < samplesToRead; ++i) {
        for (int ch = 0; ch < numChannels; ++ch) {
            inLR[i * numChannels + ch] = sourceBuffer.getSample(ch, i);
        }
    }
    
    timePitch.putInterleaved(inLR, samplesToRead);
    
    // 5. Retrieve the processed audio from time stretcher
    if (numSamples > interleavedCapacityFrames)
    {
        interleavedCapacityFrames = numSamples * 2;
        interleavedOutput.allocate((size_t)(interleavedCapacityFrames * numChannels), true);
    }
    
    float* outLR = interleavedOutput.getData();
    int produced = timePitch.receiveInterleaved(outLR, numSamples);

    // 6. De-interleave the processed audio into the output buffer
    if (produced > 0)
    {
        const int actualProduced = juce::jmin(produced, numSamples);
        for (int ch = 0; ch < juce::jmin(numChannels, audioOutBus.getNumChannels()); ++ch)
        {
            float* dest = audioOutBus.getWritePointer(ch);
            for (int i = 0; i < actualProduced; ++i)
            {
                dest[i] = outLR[i * numChannels + ch];
            }
            // Zero any remaining samples
            if (actualProduced < numSamples)
            {
                juce::FloatVectorOperations::clear(dest + actualProduced, numSamples - actualProduced);
            }
        }
    }
}

juce::ValueTree VideoFileLoaderModule::getExtraStateTree() const
{
    juce::ValueTree state("VideoFileLoaderState");
    // Save the absolute path of the currently loaded video file
    if (currentVideoFile.existsAsFile())
    {
        state.setProperty("videoFilePath", currentVideoFile.getFullPathName(), nullptr);
    }
    return state;
}

void VideoFileLoaderModule::setExtraStateTree(const juce::ValueTree& state)
{
    if (!state.hasType("VideoFileLoaderState")) return;
    
    juce::String filePath = state.getProperty("videoFilePath", "").toString();
    if (filePath.isNotEmpty())
    {
        juce::File restoredFile(filePath);
        if (restoredFile.existsAsFile())
        {
            videoFileToLoad = restoredFile;
            juce::Logger::writeToLog("[VideoFileLoader] Restored video file from preset: " + filePath);
        }
        else
        {
            juce::Logger::writeToLog("[VideoFileLoader] Warning: Saved video file not found: " + filePath);
        }
    }
}

#if defined(PRESET_CREATOR_UI)
ImVec2 VideoFileLoaderModule::getCustomNodeSize() const
{
    // Return different width based on zoom level (0=240,1=480,2=960)
    int level = zoomLevelParam ? (int) zoomLevelParam->load() : 1;
    level = juce::jlimit(0, 2, level);
    const float widths[3] { 240.0f, 480.0f, 960.0f };
    return ImVec2(widths[level], 0.0f);
}

void VideoFileLoaderModule::drawParametersInNode(float itemWidth,
                                                 const std::function<bool(const juce::String& paramId)>& isParamModulated,
                                                 const std::function<void()>& onModificationEnded)
{
    ImGui::PushItemWidth(itemWidth);
    
    if (ImGui::Button("Load Video File...", ImVec2(itemWidth, 0)))
    {
        chooseVideoFile();
    }
    
    if (currentVideoFile.existsAsFile())
    {
        ImGui::TextColored(ImVec4(0.5f, 1.0f, 0.5f, 1.0f), "%s", currentVideoFile.getFileName().toRawUTF8());
    }
    else
    {
        ImGui::TextColored(ImVec4(0.7f, 0.7f, 0.7f, 1.0f), "No file loaded");
    }
    
    bool loop = loopParam->load() > 0.5f;
    if (ImGui::Checkbox("Loop", &loop))
    {
        *dynamic_cast<juce::AudioParameterBool*>(apvts.getParameter("loop")) = loop;
        onModificationEnded();
    }
    
    ImGui::Separator();
    
    // Transport sync and play/stop controls
    bool sync = syncParam ? (*syncParam > 0.5f) : true;
    if (ImGui::Checkbox("Sync to Transport", &sync))
    {
        syncToTransport.store(sync);
        if (auto* p = dynamic_cast<juce::AudioParameterBool*>(apvts.getParameter("sync"))) *p = sync;
        if (sync)
        {
            playing.store(lastTransportPlaying.load());
            // Ensure thread will (re)open current file if needed after transport resumes
            if (currentVideoFile.existsAsFile())
                videoFileToLoad = currentVideoFile;
        }
    }

    ImGui::SameLine();
    bool localPlaying = playing.load();
    if (sync) ImGui::BeginDisabled();
    const char* btn = localPlaying ? "Stop" : "Play";
    if (sync)
    {
        // Mirror transport state in label when synced
        btn = lastTransportPlaying.load() ? "Stop" : "Play";
    }
    if (ImGui::Button(btn))
    {
        playing.store(!localPlaying);
    }
    if (sync) ImGui::EndDisabled();

    // Zoom buttons (+/-) across 3 levels
    int level = zoomLevelParam ? (int) zoomLevelParam->load() : 1;
    level = juce::jlimit(0, 2, level);
    float buttonWidth = (itemWidth / 2.0f) - 4.0f;
    const bool atMin = (level <= 0);
    const bool atMax = (level >= 2);

    if (atMin) ImGui::BeginDisabled();
    if (ImGui::Button("-", ImVec2(buttonWidth, 0)))
    {
        int newLevel = juce::jmax(0, level - 1);
        if (auto* p = apvts.getParameter("zoomLevel"))
            p->setValueNotifyingHost((float)newLevel / 2.0f);
        onModificationEnded();
    }
    if (atMin) ImGui::EndDisabled();

    ImGui::SameLine();

    if (atMax) ImGui::BeginDisabled();
    if (ImGui::Button("+", ImVec2(buttonWidth, 0)))
    {
        int newLevel = juce::jmin(2, level + 1);
        if (auto* p = apvts.getParameter("zoomLevel"))
            p->setValueNotifyingHost((float)newLevel / 2.0f);
        onModificationEnded();
    }
    if (atMax) ImGui::EndDisabled();
    
    ImGui::TextColored(ImVec4(0.7f, 0.7f, 0.7f, 1.0f), "Source ID: %d", (int)getLogicalId());
    {
        int fcc = lastFourcc.load();
        juce::String codec = fourccToString(fcc);
        juce::String friendly = fourccFriendlyName(codec);
        juce::String ext = currentVideoFile.getFileExtension();
        if (ext.startsWithChar('.')) ext = ext.substring(1);
        ImGui::TextColored(ImVec4(0.5f, 0.8f, 0.5f, 1.0f), "Codec: %s (%s)   Container: %s",
                           codec.toRawUTF8(), friendly.toRawUTF8(), (ext.isEmpty() ? "unknown" : ext.toRawUTF8()));
        if (totalFrames.load() <= 1)
            ImGui::TextColored(ImVec4(0.9f, 0.7f, 0.2f, 1.0f), "Length unknown yet (ratio seeks)");
    }

    // ADD ENGINE SELECTION COMBO BOX
    ImGui::Separator();
    int engineIdx = engineParam ? engineParam->getIndex() : 1;
    const char* items[] = { "RubberBand (High Quality)", "Naive (Low CPU)" };
    if (ImGui::Combo("Engine", &engineIdx, items, 2))
    {
        if (engineParam) *engineParam = engineIdx;
        onModificationEnded();
    }
    ImGui::Separator();

    // --- Playback speed (slider only) ---
    float spd = speedParam ? speedParam->load() : 1.0f;
    ImGui::SliderFloat("Speed", &spd, 0.25f, 4.0f, "%.2fx");
    if (auto* p = dynamic_cast<juce::AudioParameterFloat*>(apvts.getParameter("speed"))) *p = spd;

    // --- Trim / Timeline --- (always visible, never greyed)
    {
        const int tf = juce::jmax(1, totalFrames.load());
        float inN = inNormParam ? inNormParam->load() : 0.0f;
        float outN = outNormParam ? outNormParam->load() : 1.0f;
        ImGui::Separator();
        
        if (ImGui::SliderFloat("Start", &inN, 0.0f, 1.0f, "%.3f"))
        {
            inN = juce::jlimit(0.0f, outN - 0.01f, inN);
            if (auto* p = dynamic_cast<juce::AudioParameterFloat*>(apvts.getParameter("in"))) *p = inN;
            pendingSeekNormalized.store(inN); // Use unified seek
        }
        if (ImGui::IsItemDeactivatedAfterEdit()) onModificationEnded();

        if (ImGui::SliderFloat("End", &outN, 0.0f, 1.0f, "%.3f"))
        {
            outN = juce::jlimit(inN + 0.01f, 1.0f, outN);
            if (auto* p = dynamic_cast<juce::AudioParameterFloat*>(apvts.getParameter("out"))) *p = outN;
            onModificationEnded();
        }

        float pos = (tf > 1) ? ((float)lastPosFrame.load() / (float)tf) : 0.0f;
        // Clamp position UI between in/out
        float minPos = juce::jlimit(0.0f, 1.0f, inN);
        float maxPos = juce::jlimit(minPos, 1.0f, outN);
        pos = juce::jlimit(minPos, maxPos, pos);

        if (ImGui::SliderFloat("Position", &pos, 0.0f, 1.0f, "%.3f"))
        {
            pendingSeekNormalized.store(pos); // Use unified seek
        }
        if (ImGui::IsItemDeactivatedAfterEdit()) onModificationEnded();
    }
    
    ImGui::PopItemWidth();
}

void VideoFileLoaderModule::drawIoPins(const NodePinHelpers& helpers)
{
    // Although getDynamicOutputPins takes precedence, this ensures correctness if it's ever used as a fallback.
    helpers.drawAudioOutputPin("Source ID", 0);
    helpers.drawAudioOutputPin("Audio L", 1);
    helpers.drawAudioOutputPin("Audio R", 2);
}
#endif

std::vector<DynamicPinInfo> VideoFileLoaderModule::getDynamicOutputPins() const
{
    std::vector<DynamicPinInfo> pins;
    
    // CV Output: Bus 0, Channel 0 (mono - contains logical ID for video routing)
    pins.push_back({ "Source ID", 0, PinDataType::Video });
    
    // Audio Outputs: Bus 1, Channels 0-1 (stereo)
    // Note: Channel indices are absolute, so bus 1 channel 0 = absolute channel 1
    // (bus 0 has 1 channel, so bus 1 starts at absolute index 1)
    int bus1StartChannel = 1; // After bus 0's 1 channel
    pins.push_back({ "Audio L", bus1StartChannel + 0, PinDataType::Audio });
    pins.push_back({ "Audio R", bus1StartChannel + 1, PinDataType::Audio });
    
    return pins;
}



================================================================================
FILE: juce\Source\audio\modules\FFmpegAudioReader.h
================================================================================


#pragma once

#include <juce_audio_formats/juce_audio_formats.h>
#include <juce_core/juce_core.h>

// Forward-declare FFmpeg structs to keep headers clean
struct AVFormatContext;
struct AVCodecContext;
struct AVStream;
struct AVFrame;
struct AVPacket;
struct SwrContext;

/**
 * Custom audio format reader that uses FFmpeg to decode audio from video files.
 * This provides robust support for a wide range of audio codecs (AAC, AC3, etc.)
 * found in video containers (.mp4, .mkv, .mov).
 */
class FFmpegAudioReader final : public juce::AudioFormatReader
{
public:
    explicit FFmpegAudioReader(const juce::String& filePath);
    ~FFmpegAudioReader() override;

    bool readSamples(int* const* destSamples, int numDestChannels, int startOffsetInDestBuffer, juce::int64 startSampleInFile, int numSamples) override;

private:
    void cleanup();

    // FFmpeg-related members
    AVFormatContext* formatContext = nullptr;
    AVCodecContext* codecContext = nullptr;
    AVStream* audioStream = nullptr;
    SwrContext* resamplerContext = nullptr;
    AVFrame* decodedFrame = nullptr;
    AVPacket* packet = nullptr;
    
    int streamIndex = -1;
    juce::String filePath;
    bool isInitialized = false;

    // A temporary buffer for holding resampled audio data before copying to JUCE's buffers
    juce::AudioBuffer<float> tempResampledBuffer;
    
    JUCE_DECLARE_NON_COPYABLE_WITH_LEAK_DETECTOR(FFmpegAudioReader)
};


================================================================================
FILE: juce\Source\audio\modules\FFmpegAudioReader.cpp
================================================================================


#include "FFmpegAudioReader.h"
#include <mutex>

#if defined(_WIN32) || defined(_WIN64)
    #pragma warning(push)
    #pragma warning(disable: 4244 4996)
#endif

// Include FFmpeg headers
extern "C" {
#include <libavformat/avformat.h>
#include <libavcodec/avcodec.h>
#include <libavutil/opt.h>
#include <libswresample/swresample.h>
}

#if defined(_WIN32) || defined(_WIN64)
    #pragma warning(pop)
#endif

FFmpegAudioReader::FFmpegAudioReader(const juce::String& path)
    : juce::AudioFormatReader(nullptr, "FFmpeg"), filePath(path)
{
    // Initialize FFmpeg networking capabilities once.
    static std::once_flag ffmpegInitialized;
    std::call_once(ffmpegInitialized, []() { avformat_network_init(); });

    if (avformat_open_input(&formatContext, filePath.toUTF8(), nullptr, nullptr) != 0) return;
    if (avformat_find_stream_info(formatContext, nullptr) < 0) { cleanup(); return; }

    streamIndex = av_find_best_stream(formatContext, AVMEDIA_TYPE_AUDIO, -1, -1, nullptr, 0);
    if (streamIndex < 0) { cleanup(); return; }

    audioStream = formatContext->streams[streamIndex];
    const AVCodec* codec = avcodec_find_decoder(audioStream->codecpar->codec_id);
    if (!codec) { cleanup(); return; }

    codecContext = avcodec_alloc_context3(codec);
    if (!codecContext) { cleanup(); return; }
    
    if (avcodec_parameters_to_context(codecContext, audioStream->codecpar) < 0) { cleanup(); return; }
    if (avcodec_open2(codecContext, codec, nullptr) < 0) { cleanup(); return; }

    // Set up the JUCE AudioFormatReader properties based on the decoded stream
    this->sampleRate = (double)codecContext->sample_rate;
    this->numChannels = (unsigned int)codecContext->ch_layout.nb_channels;
    this->bitsPerSample = 32;
    this->usesFloatingPointData = true; // We will provide float data directly

    // Calculate total audio duration in samples
    if (audioStream->duration != AV_NOPTS_VALUE) {
        double durationInSeconds = (double)audioStream->duration * av_q2d(audioStream->time_base);
        this->lengthInSamples = (juce::int64)(durationInSeconds * this->sampleRate);
    }

    // Configure the resampler to convert FFmpeg's decoded audio format to 32-bit float
    resamplerContext = swr_alloc();
    if (!resamplerContext) { cleanup(); return; }
    
    av_opt_set_chlayout(resamplerContext, "in_chlayout", &codecContext->ch_layout, 0);
    av_opt_set_int(resamplerContext, "in_sample_rate", codecContext->sample_rate, 0);
    av_opt_set_sample_fmt(resamplerContext, "in_sample_fmt", codecContext->sample_fmt, 0);
    av_opt_set_chlayout(resamplerContext, "out_chlayout", &codecContext->ch_layout, 0);
    av_opt_set_int(resamplerContext, "out_sample_rate", (int)this->sampleRate, 0);
    av_opt_set_sample_fmt(resamplerContext, "out_sample_fmt", AV_SAMPLE_FMT_FLTP, 0);  // Planar float for JUCE
    
    if (swr_init(resamplerContext) < 0) { cleanup(); return; }

    decodedFrame = av_frame_alloc();
    packet = av_packet_alloc();
    isInitialized = (decodedFrame && packet);
}

FFmpegAudioReader::~FFmpegAudioReader() {
    cleanup();
}

void FFmpegAudioReader::cleanup() {
    if (resamplerContext) swr_free(&resamplerContext);
    if (decodedFrame) av_frame_free(&decodedFrame);
    if (packet) av_packet_free(&packet);
    if (codecContext) avcodec_free_context(&codecContext);
    if (formatContext) avformat_close_input(&formatContext);
}

bool FFmpegAudioReader::readSamples(int* const* destSamples, int numDestChannels, int startOffsetInDestBuffer, juce::int64 startSampleInFile, int numSamples)
{
    if (!isInitialized || numSamples <= 0) return true;

    // Correctly interpret the destination buffer as float** since usesFloatingPointData is true.
    auto* floatDestSamples = reinterpret_cast<float* const*>(destSamples);
    
    // Clear destination buffers to ensure silence if we fail to read
    for (int i = 0; i < numDestChannels; ++i) {
        if (floatDestSamples[i] != nullptr) {
            juce::FloatVectorOperations::clear(floatDestSamples[i] + startOffsetInDestBuffer, numSamples);
        }
    }

    // Seek to the requested position within the audio stream. This is a crucial step.
    int64_t targetTimestamp = (int64_t)((double)startSampleInFile / sampleRate / av_q2d(audioStream->time_base));
    if (av_seek_frame(formatContext, streamIndex, targetTimestamp, AVSEEK_FLAG_BACKWARD) < 0) {
        return false;
    }
    avcodec_flush_buffers(codecContext);

    int samplesWritten = 0;
    while (samplesWritten < numSamples)
    {
        if (av_read_frame(formatContext, packet) < 0) break; // End of file

        if (packet->stream_index == streamIndex)
        {
            if (avcodec_send_packet(codecContext, packet) == 0)
            {
                while (avcodec_receive_frame(codecContext, decodedFrame) == 0)
                {
                    int maxOutSamples = (int)av_rescale_rnd(decodedFrame->nb_samples, (int)this->sampleRate, codecContext->sample_rate, AV_ROUND_UP);
                    tempResampledBuffer.setSize((int)this->numChannels, maxOutSamples, false, false, true);
                    
                    // Prepare output pointers for planar format (one pointer per channel)
                    uint8_t* outData[64];  // FFmpeg supports up to 64 channels
                    for (int ch = 0; ch < (int)this->numChannels; ++ch) {
                        outData[ch] = (uint8_t*)tempResampledBuffer.getWritePointer(ch);
                    }
                    
                    int samplesConverted = swr_convert(resamplerContext, outData, maxOutSamples, (const uint8_t**)decodedFrame->extended_data, decodedFrame->nb_samples);
                    
                    if (samplesConverted > 0)
                    {
                        int samplesToCopy = std::min(samplesConverted, numSamples - samplesWritten);
                        for (int ch = 0; ch < std::min((int)this->numChannels, numDestChannels); ++ch) {
                            if (floatDestSamples[ch] != nullptr) {
                                // Directly copy the decoded float samples into the destination buffer
                                juce::FloatVectorOperations::copy(floatDestSamples[ch] + startOffsetInDestBuffer + samplesWritten,
                                                                  tempResampledBuffer.getReadPointer(ch),
                                                                  samplesToCopy);
                            }
                        }
                        samplesWritten += samplesToCopy;
                    }
                }
            }
        }
        av_packet_unref(packet);
        if (samplesWritten >= numSamples) break;
    }

    return true;
}

